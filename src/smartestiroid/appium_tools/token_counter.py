"""
Token counting and cost calculation functionality using tiktoken
OpenAI APIã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°è¨ˆç®—ã¨è²»ç”¨è¨ˆç®—æ©Ÿèƒ½
"""
from typing import Any, Dict, List, Optional, Tuple
from contextlib import contextmanager
from langchain_core.callbacks.base import BaseCallbackHandler


class OpenAIPricingCalculator:
    """OpenAI APIã®æ–™é‡‘è¨ˆç®—ã‚¯ãƒ©ã‚¹"""
    
    # OpenAI pricing (USD per 1K tokens) - 2025å¹´11æœˆ24æ—¥æ™‚ç‚¹ã®æœ€æ–°æ–™é‡‘
    # https://platform.openai.com/docs/pricing
    PRICING = {
        # Latest GPT-5 series (æœ€æ–°ãƒ¢ãƒ‡ãƒ«)
        "gpt-5.1": {
            "input": 0.00125,  # $1.25 / 1M tokens
            "cached": 0.000125,  # $0.125 / 1M tokens (10%)
            "output": 0.010,   # $10.00 / 1M tokens
        },
        "gpt-5": {
            "input": 0.00125,  # $1.25 / 1M tokens
            "cached": 0.000125,  # $0.125 / 1M tokens (10%)
            "output": 0.010,   # $10.00 / 1M tokens
        },
        "gpt-5-mini": {
            "input": 0.00025,  # $0.25 / 1M tokens
            "cached": 0.000025,  # $0.025 / 1M tokens (10%)
            "output": 0.002,   # $2.00 / 1M tokens
        },
        "gpt-5-nano": {
            "input": 0.00005,  # $0.05 / 1M tokens
            "cached": 0.000005,  # $0.005 / 1M tokens (10%)
            "output": 0.0004,  # $0.40 / 1M tokens
        },
        "gpt-5.1-chat-latest": {
            "input": 0.00125,  # $1.25 / 1M tokens
            "cached": 0.000125,  # $0.125 / 1M tokens (10%)
            "output": 0.010,   # $10.00 / 1M tokens
        },
        "gpt-5-chat-latest": {
            "input": 0.00125,  # $1.25 / 1M tokens
            "cached": 0.000125,  # $0.125 / 1M tokens (10%)
            "output": 0.010,   # $10.00 / 1M tokens
        },
        "gpt-5.1-codex": {
            "input": 0.00125,  # $1.25 / 1M tokens
            "cached": 0.000125,  # $0.125 / 1M tokens (10%)
            "output": 0.010,   # $10.00 / 1M tokens
        },
        "gpt-5-codex": {
            "input": 0.00125,  # $1.25 / 1M tokens
            "cached": 0.000125,  # $0.125 / 1M tokens (10%)
            "output": 0.010,   # $10.00 / 1M tokens
        },
        "gpt-5-pro": {
            "input": 0.015,    # $15.00 / 1M tokens
            "cached": 0.015,   # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãªã—
            "output": 0.120,   # $120.00 / 1M tokens
        },
        
        # GPT-4.1 series (æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«)
        "gpt-4.1": {
            "input": 0.002,    # $2.00 / 1M tokens
            "cached": 0.0005,  # $0.50 / 1M tokens (25%)
            "output": 0.008,   # $8.00 / 1M tokens
        },
        "gpt-4.1-mini": {
            "input": 0.0004,   # $0.40 / 1M tokens
            "cached": 0.0001,  # $0.10 / 1M tokens (25%)
            "output": 0.0016,  # $1.60 / 1M tokens
        },
        "gpt-4.1-nano": {
            "input": 0.0001,   # $0.10 / 1M tokens
            "cached": 0.000025,  # $0.025 / 1M tokens (25%)
            "output": 0.0004,  # $0.40 / 1M tokens
        },
        
        # O-series models (æ¨è«–ãƒ¢ãƒ‡ãƒ«)
        "o1": {
            "input": 0.015,    # $15.00 / 1M tokens
            "cached": 0.0075,  # $7.50 / 1M tokens (50%)
            "output": 0.060,   # $60.00 / 1M tokens
        },
        "o1-pro": {
            "input": 0.150,    # $150.00 / 1M tokens
            "cached": 0.150,   # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãªã—
            "output": 0.600,   # $600.00 / 1M tokens
        },
        "o3": {
            "input": 0.002,    # $2.00 / 1M tokens
            "cached": 0.0005,  # $0.50 / 1M tokens (25%)
            "output": 0.008,   # $8.00 / 1M tokens
        },
        "o3-pro": {
            "input": 0.020,    # $20.00 / 1M tokens
            "cached": 0.020,   # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãªã—
            "output": 0.080,   # $80.00 / 1M tokens
        },
        "o3-deep-research": {
            "input": 0.010,    # $10.00 / 1M tokens
            "cached": 0.0025,  # $2.50 / 1M tokens (25%)
            "output": 0.040,   # $40.00 / 1M tokens
        },
        "o4-mini": {
            "input": 0.0011,   # $1.10 / 1M tokens
            "cached": 0.000275,  # $0.275 / 1M tokens (25%)
            "output": 0.0044,  # $4.40 / 1M tokens
        },
        "o4-mini-deep-research": {
            "input": 0.002,    # $2.00 / 1M tokens
            "cached": 0.0005,  # $0.50 / 1M tokens (25%)
            "output": 0.008,   # $8.00 / 1M tokens
        },
        "o3-mini": {
            "input": 0.0011,   # $1.10 / 1M tokens
            "cached": 0.00055,  # $0.55 / 1M tokens (50%)
            "output": 0.0044,  # $4.40 / 1M tokens
        },
        "o1-mini": {
            "input": 0.0011,   # $1.10 / 1M tokens
            "cached": 0.00055,  # $0.55 / 1M tokens (50%)
            "output": 0.0044,  # $4.40 / 1M tokens
        },
        
        # GPT-4o models (ç¾è¡Œãƒ¢ãƒ‡ãƒ«)
        "gpt-4o": {
            "input": 0.0025,   # $2.50 / 1M tokens
            "cached": 0.00125,  # $1.25 / 1M tokens (50%)
            "output": 0.010,   # $10.00 / 1M tokens
        },
        "gpt-4o-mini": {
            "input": 0.000150, # $0.15 / 1M tokens
            "cached": 0.000075, # $0.075 / 1M tokens (50%)
            "output": 0.000600, # $0.60 / 1M tokens
        },
        "gpt-4o-2024-05-13": {
            "input": 0.005,    # $5.00 / 1M tokens
            "cached": 0.005,   # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãªã—
            "output": 0.015,   # $15.00 / 1M tokens
        },
        
        # Realtime models
        "gpt-realtime": {
            "input": 0.004,    # $4.00 / 1M tokens
            "cached": 0.0004,  # $0.40 / 1M tokens (10%)
            "output": 0.016,   # $16.00 / 1M tokens
        },
        "gpt-realtime-mini": {
            "input": 0.0006,   # $0.60 / 1M tokens
            "cached": 0.00006,  # $0.06 / 1M tokens (10%)
            "output": 0.0024,  # $2.40 / 1M tokens
        },
        
        # Legacy models for backward compatibility
        "gpt-4": {
            "input": 0.03,     # $30.00 / 1M tokens
            "cached": 0.03,    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãªã—
            "output": 0.06,    # $60.00 / 1M tokens
        },
        "gpt-4-32k": {
            "input": 0.06,     # $60.00 / 1M tokens
            "cached": 0.06,    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãªã—
            "output": 0.12,    # $120.00 / 1M tokens
        },
        "gpt-4-turbo": {
            "input": 0.01,     # $10.00 / 1M tokens
            "cached": 0.01,    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãªã—
            "output": 0.03,    # $30.00 / 1M tokens
        },
        "gpt-3.5-turbo": {
            "input": 0.0005,   # $0.50 / 1M tokens
            "cached": 0.0005,  # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãªã—
            "output": 0.0015,  # $1.50 / 1M tokens
        },
        "gpt-3.5-turbo-16k": {
            "input": 0.003,    # $3.00 / 1M tokens
            "cached": 0.003,   # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãªã—
            "output": 0.004,   # $4.00 / 1M tokens
        },
        
        # Fallback for unknown models - use gpt-4.1-mini pricing (cost-effective)
        "default": {
            "input": 0.0004,
            "cached": 0.0001,
            "output": 0.0016,
        }
    }
    
    @classmethod
    def calculate_cost(cls, model_name: str, input_tokens: int, output_tokens: int) -> Dict[str, float]:
        """
        ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‹ã‚‰è²»ç”¨ã‚’è¨ˆç®—ã™ã‚‹
        
        Args:
            model_name: OpenAIãƒ¢ãƒ‡ãƒ«å
            input_tokens: å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ•°
            output_tokens: å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ•°
            
        Returns:
            Dict containing input_cost, output_cost, total_cost in USD
        """
        # ãƒ¢ãƒ‡ãƒ«åã‚’æ­£è¦åŒ–ï¼ˆãƒãƒ¼ã‚¸ãƒ§ãƒ³ç•ªå·ç­‰ã‚’é™¤å»ï¼‰
        normalized_model = cls._normalize_model_name(model_name)
        
        # æ–™é‡‘æƒ…å ±ã‚’å–å¾—ï¼ˆæœªçŸ¥ã®ãƒ¢ãƒ‡ãƒ«ã¯defaultã‚’ä½¿ç”¨ï¼‰
        pricing = cls.PRICING.get(normalized_model, cls.PRICING["default"])
        
        # è²»ç”¨è¨ˆç®—ï¼ˆ1K tokenså˜ä½ã§ã®æ–™é‡‘ãªã®ã§ã€1000ã§å‰²ã‚‹ï¼‰
        input_cost = (input_tokens / 1000) * pricing["input"]
        output_cost = (output_tokens / 1000) * pricing["output"]
        total_cost = input_cost + output_cost
        
        return {
            "input_cost": round(input_cost, 6),
            "output_cost": round(output_cost, 6),
            "total_cost": round(total_cost, 6)
        }
    
    @classmethod
    def _normalize_model_name(cls, model_name: str) -> str:
        """
        ãƒ¢ãƒ‡ãƒ«åã‚’æ­£è¦åŒ–ã—ã¦æ–™é‡‘è¡¨ã®ã‚­ãƒ¼ã¨ä¸€è‡´ã•ã›ã‚‹
        """
        model_lower = model_name.lower().strip()
        
        # Exact matches first
        if model_lower in cls.PRICING:
            return model_lower
        
        # Pattern matching for versioned models (æœ€æ–°ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰é †ç•ªã«ãƒã‚§ãƒƒã‚¯)
        # GPT-5 series
        if "gpt-5-pro" in model_lower:
            return "gpt-5-pro"
        elif "gpt-5-nano" in model_lower:
            return "gpt-5-nano"
        elif "gpt-5-mini" in model_lower:
            return "gpt-5-mini"
        elif "gpt-5-chat-latest" in model_lower:
            return "gpt-5-chat-latest"
        elif "gpt-5-codex" in model_lower:
            return "gpt-5-codex"
        elif "gpt-5" in model_lower:
            return "gpt-5"
        
        # GPT-4.1 series
        elif "gpt-4.1-nano" in model_lower:
            return "gpt-4.1-nano"
        elif "gpt-4.1-mini" in model_lower:
            return "gpt-4.1-mini"
        elif "gpt-4.1" in model_lower:
            return "gpt-4.1"
        
        # O-series models
        elif "o4-mini-deep-research" in model_lower:
            return "o4-mini-deep-research"
        elif "o4-mini" in model_lower:
            return "o4-mini"
        elif "o3-deep-research" in model_lower:
            return "o3-deep-research"
        elif "o3-pro" in model_lower:
            return "o3-pro"
        elif "o3-mini" in model_lower:
            return "o3-mini"
        elif "o3" in model_lower:
            return "o3"
        elif "o1-pro" in model_lower:
            return "o1-pro"
        elif "o1-mini" in model_lower:
            return "o1-mini"
        elif "o1" in model_lower:
            return "o1"
        
        # GPT-4o series
        elif "gpt-4o-mini" in model_lower:
            return "gpt-4o-mini"
        elif "gpt-4o-2024-05-13" in model_lower:
            return "gpt-4o-2024-05-13"
        elif "gpt-4o" in model_lower:
            return "gpt-4o"
        
        # Realtime models
        elif "gpt-realtime-mini" in model_lower:
            return "gpt-realtime-mini"
        elif "gpt-realtime" in model_lower:
            return "gpt-realtime"
        
        # Legacy GPT-4 models
        elif "gpt-4-turbo" in model_lower:
            return "gpt-4-turbo"
        elif "gpt-4-32k" in model_lower:
            return "gpt-4-32k"
        elif "gpt-4" in model_lower:
            return "gpt-4"
        
        # GPT-3.5 models
        elif "gpt-3.5-turbo-16k" in model_lower:
            return "gpt-3.5-turbo-16k"
        elif "gpt-3.5-turbo" in model_lower:
            return "gpt-3.5-turbo"
        
        else:
            return "default"


class TiktokenCountCallback(BaseCallbackHandler):
    """
    LangChain callback to count tokens using tiktoken
    tiktoken ã‚’ä½¿ç”¨ã—ã¦ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’è¨ˆç®—ã™ã‚‹LangChainã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
    
    å„ainvokeå‘¼ã³å‡ºã—ã”ã¨ã®è©³ç´°ãªå±¥æ­´ã‚’ä¿å­˜ã—ã€å¾Œã‹ã‚‰å–ã‚Šå‡ºã›ã¾ã™ã€‚
    
    ã‚°ãƒ­ãƒ¼ãƒãƒ«çµ±è¨ˆæ©Ÿèƒ½:
    - è¤‡æ•°ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ï¼ˆappium_driverã®èµ·å‹•ï¼‰ã‚’ã¾ãŸã„ã ç´¯ç©çµ±è¨ˆã‚’ä¿æŒ
    - reset_counters()ã‚’å‘¼ã‚“ã§ã‚‚ã€ã‚°ãƒ­ãƒ¼ãƒãƒ«çµ±è¨ˆã¯ä¿æŒã•ã‚Œã‚‹
    - save_session_to_global()ã§ç¾åœ¨ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ã‚°ãƒ­ãƒ¼ãƒãƒ«å±¥æ­´ã«è¿½åŠ 
    """
    
    # ã‚¯ãƒ©ã‚¹å¤‰æ•°: å…¨ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ãƒ»å…¨ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’é€šã˜ãŸç´¯ç©å±¥æ­´
    _global_history: List[Dict[str, Any]] = []
    
    def __init__(self, model: str = "gpt-4.1-mini") -> None:
        """
        Initialize the callback with the specified model
        
        Args:
            model: OpenAI model name for token encoding
        """
        self.model = model
        self.input_tokens = 0
        self.cached_tokens = 0  # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆã—ãŸãƒˆãƒ¼ã‚¯ãƒ³æ•°
        self.output_tokens = 0
        self.pricing_calculator = OpenAIPricingCalculator()
        
        # ainvokeã”ã¨ã®å±¥æ­´ã‚’ä¿å­˜ã™ã‚‹ãƒªã‚¹ãƒˆï¼ˆã‚»ãƒƒã‚·ãƒ§ãƒ³å˜ä½ï¼‰
        self.invocation_history: List[Dict[str, Any]] = []
        self._current_invocation_id = 0
    
    def on_llm_start(self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any) -> None:
        """LLMé–‹å§‹æ™‚ã«å‘¼ã³å‡ºã•ã‚Œã‚‹ - æ–°ã—ã„invocationã®é–‹å§‹ã‚’è¨˜éŒ²"""
        self._current_invocation_id += 1
        self._current_invocation_start_time = __import__('time').time()
    
    def on_llm_new_token(self, token: str, **kwargs: Any) -> None:
        """ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°æ™‚ã«å‘¼ã³å‡ºã•ã‚Œã‚‹ï¼ˆä½•ã‚‚ã—ãªã„ï¼‰"""
        pass
    
    def on_llm_end(self, response, **kwargs: Any) -> None:
        """
        Called when LLM completes - count tokens from actual API response
        LLMå®Œäº†æ™‚ã«å‘¼ã³å‡ºã•ã‚Œã€å®Ÿéš›ã®APIãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’å–å¾—ã—ã€å±¥æ­´ã«è¨˜éŒ²
        """
        if not (hasattr(response, 'llm_output') and response.llm_output):
            raise ValueError("APIãƒ¬ã‚¹ãƒãƒ³ã‚¹ã«llm_outputãŒå«ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
        
        token_usage = response.llm_output.get('token_usage')
        if not token_usage:
            raise ValueError("APIãƒ¬ã‚¹ãƒãƒ³ã‚¹ã«token_usageãŒå«ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
        
        # OpenAI APIã®å®Ÿéš›ã®ä½¿ç”¨é‡ã‚’ä½¿ç”¨
        prompt_tokens = token_usage.get('prompt_tokens', 0)
        completion_tokens = token_usage.get('completion_tokens', 0)
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ã‚’å–å¾—ï¼ˆ50%å‰²å¼•é©ç”¨ï¼‰
        prompt_details = token_usage.get('prompt_tokens_details', {})
        cached_tokens = prompt_details.get('cached_tokens', 0)
        
        # é€šå¸¸ãƒˆãƒ¼ã‚¯ãƒ³ã¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒˆãƒ¼ã‚¯ãƒ³ã‚’åˆ†ã‘ã¦è¨˜éŒ²
        self.input_tokens += prompt_tokens
        self.cached_tokens = getattr(self, 'cached_tokens', 0) + cached_tokens
        self.output_tokens += completion_tokens
        
        # ã“ã®invocationã®è²»ç”¨ã‚’è¨ˆç®—
        invocation_cost = self._calculate_invocation_cost(
            prompt_tokens, cached_tokens, completion_tokens
        )
        
        # å±¥æ­´ã«è¨˜éŒ²
        elapsed_time = __import__('time').time() - getattr(self, '_current_invocation_start_time', __import__('time').time())
        invocation_record = {
            "invocation_id": self._current_invocation_id,
            "timestamp": __import__('datetime').datetime.now().isoformat(),
            "elapsed_seconds": round(elapsed_time, 2),
            "model": self.model,
            "input_tokens": prompt_tokens,
            "cached_tokens": cached_tokens,
            "output_tokens": completion_tokens,
            "total_tokens": prompt_tokens + completion_tokens,
            "input_cost_usd": invocation_cost["input_cost"],
            "output_cost_usd": invocation_cost["output_cost"],
            "cached_cost_usd": invocation_cost["cached_cost"],
            "total_cost_usd": invocation_cost["total_cost"],
        }
        self.invocation_history.append(invocation_record)
    
    def _calculate_invocation_cost(self, input_tokens: int, cached_tokens: int, output_tokens: int) -> Dict[str, float]:
        """
        å˜ä¸€invocationã®è²»ç”¨ã‚’è¨ˆç®—
        """
        normalized_model = self.pricing_calculator._normalize_model_name(self.model)
        pricing = self.pricing_calculator.PRICING.get(normalized_model, self.pricing_calculator.PRICING["default"])
        
        # é€šå¸¸ã®å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚Œã¦ã„ãªã„éƒ¨åˆ†ï¼‰
        non_cached_tokens = input_tokens - cached_tokens
        
        # è²»ç”¨è¨ˆç®—
        non_cached_cost = (non_cached_tokens / 1000) * pricing["input"]
        cached_cost = (cached_tokens / 1000) * pricing["cached"]
        output_cost = (output_tokens / 1000) * pricing["output"]
        
        input_cost = non_cached_cost + cached_cost
        total_cost = input_cost + output_cost
        
        return {
            "input_cost": round(input_cost, 6),
            "output_cost": round(output_cost, 6),
            "cached_cost": round(cached_cost, 6),
            "total_cost": round(total_cost, 6),
        }
    
    @property
    def total_tokens(self) -> int:
        """Total tokens used (input + output)"""
        return self.input_tokens + self.output_tokens
    
    def get_cost_breakdown(self) -> Dict[str, float]:
        """
        Calculate the cost breakdown for the tokens used
        ä½¿ç”¨ã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ã®è²»ç”¨å†…è¨³ã‚’è¨ˆç®—ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥å‰²å¼•ã‚’è€ƒæ…®ï¼‰
        """
        # ãƒ¢ãƒ‡ãƒ«åã‚’æ­£è¦åŒ–
        normalized_model = self.pricing_calculator._normalize_model_name(self.model)
        pricing = self.pricing_calculator.PRICING.get(normalized_model, self.pricing_calculator.PRICING["default"])
        
        # é€šå¸¸ã®å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚Œã¦ã„ãªã„éƒ¨åˆ†ï¼‰
        non_cached_tokens = self.input_tokens - self.cached_tokens
        
        # è²»ç”¨è¨ˆç®—
        # é€šå¸¸ã®å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³: é€šå¸¸æ–™é‡‘
        non_cached_cost = (non_cached_tokens / 1000) * pricing["input"]
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆãƒˆãƒ¼ã‚¯ãƒ³: ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ–™é‡‘ï¼ˆãƒ¢ãƒ‡ãƒ«ã”ã¨ã«ç•°ãªã‚‹ï¼‰
        cached_cost = (self.cached_tokens / 1000) * pricing["cached"]
        # å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³: é€šå¸¸æ–™é‡‘
        output_cost = (self.output_tokens / 1000) * pricing["output"]
        
        input_cost = non_cached_cost + cached_cost
        total_cost = input_cost + output_cost
        
        return {
            "input_cost": round(input_cost, 6),
            "output_cost": round(output_cost, 6),
            "total_cost": round(total_cost, 6),
            "cached_cost": round(cached_cost, 6),
        }
    
    def get_metrics(self) -> Dict[str, Any]:
        """
        Get comprehensive metrics including tokens and costs
        ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã¨è²»ç”¨ã‚’å«ã‚€ç·åˆçš„ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å–å¾—
        """
        cost_breakdown = self.get_cost_breakdown()
        
        return {
            "model": self.model,
            "input_tokens": self.input_tokens,
            "cached_tokens": self.cached_tokens,  # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆæ•°ã‚’è¿½åŠ 
            "output_tokens": self.output_tokens,
            "total_tokens": self.total_tokens,
            "input_cost_usd": cost_breakdown["input_cost"],
            "output_cost_usd": cost_breakdown["output_cost"],
            "total_cost_usd": cost_breakdown["total_cost"],
            "cached_cost_usd": cost_breakdown["cached_cost"],  # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚³ã‚¹ãƒˆã‚’è¿½åŠ 
        }
    
    def reset_counters(self) -> None:
        """
        Reset all counters for reuse
        ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ã‚’ãƒªã‚»ãƒƒãƒˆã—ã¦å†åˆ©ç”¨å¯èƒ½ã«ã™ã‚‹
        
        æ³¨æ„: ã‚°ãƒ­ãƒ¼ãƒãƒ«å±¥æ­´(_global_history)ã¯ãƒªã‚»ãƒƒãƒˆã•ã‚Œã¾ã›ã‚“
        """
        self.input_tokens = 0
        self.cached_tokens = 0
        self.output_tokens = 0
        self.invocation_history.clear()
        self._current_invocation_id = 0
    
    def get_invocation_history(self) -> List[Dict[str, Any]]:
        """
        å…¨ã¦ã®ainvokeå‘¼ã³å‡ºã—å±¥æ­´ã‚’å–å¾—
        
        Returns:
            List of invocation records with tokens, costs, and metadata
        """
        return self.invocation_history.copy()
    
    def get_invocation_by_id(self, invocation_id: int) -> Optional[Dict[str, Any]]:
        """
        ç‰¹å®šã®invocation IDã®æƒ…å ±ã‚’å–å¾—
        
        Args:
            invocation_id: The invocation ID to retrieve
            
        Returns:
            Invocation record or None if not found
        """
        for record in self.invocation_history:
            if record["invocation_id"] == invocation_id:
                return record.copy()
        return None
    
    def get_latest_invocation(self) -> Optional[Dict[str, Any]]:
        """
        æœ€æ–°ã®ainvokeå‘¼ã³å‡ºã—æƒ…å ±ã‚’å–å¾—
        
        Returns:
            Latest invocation record or None if no invocations yet
        """
        if not self.invocation_history:
            return None
        return self.invocation_history[-1].copy()
    
    def get_invocations_summary(self) -> Dict[str, Any]:
        """
        å…¨ã¦ã®ainvokeå‘¼ã³å‡ºã—ã®ã‚µãƒãƒªãƒ¼ã‚’å–å¾—
        
        Returns:
            Summary including count, total tokens, and total cost
        """
        if not self.invocation_history:
            return {
                "total_invocations": 0,
                "total_input_tokens": 0,
                "total_cached_tokens": 0,
                "total_output_tokens": 0,
                "total_tokens": 0,
                "total_cost_usd": 0.0,
                "average_tokens_per_invocation": 0.0,
                "average_cost_per_invocation": 0.0,
            }
        
        total_input = sum(r["input_tokens"] for r in self.invocation_history)
        total_cached = sum(r["cached_tokens"] for r in self.invocation_history)
        total_output = sum(r["output_tokens"] for r in self.invocation_history)
        total_cost = sum(r["total_cost_usd"] for r in self.invocation_history)
        count = len(self.invocation_history)
        
        return {
            "total_invocations": count,
            "total_input_tokens": total_input,
            "total_cached_tokens": total_cached,
            "total_output_tokens": total_output,
            "total_tokens": total_input + total_output,
            "total_cost_usd": round(total_cost, 6),
            "average_tokens_per_invocation": round((total_input + total_output) / count, 2),
            "average_cost_per_invocation": round(total_cost / count, 6),
        }
    
    def format_invocation_details(self, width: int = 70) -> str:
        """
        å„LLMå‘¼ã³å‡ºã—ã®è©³ç´°ã‚’æ•´å½¢ã•ã‚ŒãŸæ–‡å­—åˆ—ã§è¿”ã™
        
        Args:
            width: è¡¨ç¤ºå¹…ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 70æ–‡å­—ï¼‰
            
        Returns:
            æ•´å½¢ã•ã‚ŒãŸè©³ç´°æƒ…å ±ã®æ–‡å­—åˆ—
        """
        if not self.invocation_history:
            return "No LLM invocations recorded yet."
        
        lines = []
        lines.append("=" * width)
        lines.append("ğŸ“Š LLM Invocation Details:")
        lines.append("=" * width)
        
        for inv in self.invocation_history:
            lines.append(f"\nğŸ”¹ Call #{inv['invocation_id']} ({inv['elapsed_seconds']}s)")
            lines.append(f"   Tokens: {inv['input_tokens']} input + {inv['output_tokens']} output = {inv['total_tokens']} total")
            if inv['cached_tokens'] > 0:
                lines.append(f"   ğŸ’¾ Cache Hit: {inv['cached_tokens']} tokens saved ${inv['cached_cost_usd']:.6f}")
            lines.append(f"   ğŸ’° Cost: ${inv['total_cost_usd']:.6f}")
        
        return "\n".join(lines)
    
    def format_summary(self, width: int = 70) -> str:
        """
        ã‚µãƒãƒªãƒ¼çµ±è¨ˆã‚’æ•´å½¢ã•ã‚ŒãŸæ–‡å­—åˆ—ã§è¿”ã™
        
        Args:
            width: è¡¨ç¤ºå¹…ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 70æ–‡å­—ï¼‰
            
        Returns:
            æ•´å½¢ã•ã‚ŒãŸã‚µãƒãƒªãƒ¼æƒ…å ±ã®æ–‡å­—åˆ—
        """
        summary = self.get_invocations_summary()
        
        if summary['total_invocations'] == 0:
            return "No LLM invocations to summarize."
        
        lines = []
        lines.append("=" * width)
        lines.append("ğŸ“ˆ Summary:")
        lines.append("=" * width)
        lines.append(f"Total LLM Calls: {summary['total_invocations']}")
        lines.append(f"Total Tokens: {summary['total_tokens']} ({summary['total_input_tokens']} input + {summary['total_output_tokens']} output)")
        if summary['total_cached_tokens'] > 0:
            lines.append(f"ğŸ’¾ Total Cached: {summary['total_cached_tokens']} tokens")
        lines.append(f"ğŸ’° Total Cost: ${summary['total_cost_usd']:.6f}")
        lines.append(f"ğŸ“Š Average: {summary['average_tokens_per_invocation']:.1f} tokens/call, ${summary['average_cost_per_invocation']:.6f}/call")
        lines.append("=" * width)
        
        return "\n".join(lines)
    
    def format_report(self, width: int = 70, show_details: bool = True) -> str:
        """
        è©³ç´°ã¨ã‚µãƒãƒªãƒ¼ã‚’å«ã‚€å®Œå…¨ãªãƒ¬ãƒãƒ¼ãƒˆã‚’æ•´å½¢ã•ã‚ŒãŸæ–‡å­—åˆ—ã§è¿”ã™
        
        Args:
            width: è¡¨ç¤ºå¹…ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 70æ–‡å­—ï¼‰
            show_details: è©³ç´°ã‚’è¡¨ç¤ºã™ã‚‹ã‹ã©ã†ã‹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: Trueï¼‰
            
        Returns:
            æ•´å½¢ã•ã‚ŒãŸå®Œå…¨ãªãƒ¬ãƒãƒ¼ãƒˆã®æ–‡å­—åˆ—
        """
        if not self.invocation_history:
            return "No LLM invocations recorded yet."
        
        parts = []
        
        if show_details:
            parts.append(self.format_invocation_details(width))
            parts.append("")  # ç©ºè¡Œ
        
        parts.append(self.format_summary(width))
        
        return "\n".join(parts)
    
    def format_loop_report(self, start_index: int, width: int = 70) -> str:
        """
        ç‰¹å®šã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä»¥é™ã®invocationã®ã¿ã®ãƒ¬ãƒãƒ¼ãƒˆã‚’æ•´å½¢ã—ã¦è¿”ã™ï¼ˆãƒ«ãƒ¼ãƒ—ã”ã¨ã®è¡¨ç¤ºç”¨ï¼‰
        
        Args:
            start_index: é–‹å§‹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆã“ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä»¥é™ã®invocationã‚’è¡¨ç¤ºï¼‰
            width: è¡¨ç¤ºå¹…ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 70æ–‡å­—ï¼‰
            
        Returns:
            æ•´å½¢ã•ã‚ŒãŸãƒ«ãƒ¼ãƒ—ãƒ¬ãƒãƒ¼ãƒˆã®æ–‡å­—åˆ—
        """
        if not self.invocation_history or start_index >= len(self.invocation_history):
            return ""
        
        loop_history = self.invocation_history[start_index:]
        
        lines = []
        lines.append("=" * width)
        lines.append("ğŸ“Š This Query LLM Calls:")
        lines.append("=" * width)
        
        loop_input_tokens = 0
        loop_cached_tokens = 0
        loop_output_tokens = 0
        loop_cost = 0.0
        
        for inv in loop_history:
            lines.append(f"\nğŸ”¹ Call #{inv['invocation_id']} ({inv['elapsed_seconds']}s)")
            lines.append(f"   Model: {inv['model']}")
            lines.append(f"   Tokens: {inv['input_tokens']} input + {inv['output_tokens']} output = {inv['total_tokens']} total")
            if inv['cached_tokens'] > 0:
                lines.append(f"   ğŸ’¾ Cache Hit: {inv['cached_tokens']} tokens saved ${inv['cached_cost_usd']:.6f}")
            lines.append(f"   ğŸ’° Cost: ${inv['total_cost_usd']:.6f}")
            
            loop_input_tokens += inv['input_tokens']
            loop_cached_tokens += inv['cached_tokens']
            loop_output_tokens += inv['output_tokens']
            loop_cost += inv['total_cost_usd']
        
        lines.append("\n" + "-" * width)
        lines.append(f"ğŸ“Š This Query Total: {len(loop_history)} calls, {loop_input_tokens + loop_output_tokens} tokens, ${loop_cost:.6f}")
        lines.append("=" * width)
        
        return "\n".join(lines)
    
    def format_session_summary(self, width: int = 70) -> str:
        """
        ã‚»ãƒƒã‚·ãƒ§ãƒ³å…¨ä½“ã®ã‚µãƒãƒªãƒ¼ã‚’æ•´å½¢ã—ã¦è¿”ã™ï¼ˆquitæ™‚ã®è¡¨ç¤ºç”¨ï¼‰
        
        Args:
            width: è¡¨ç¤ºå¹…ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 70æ–‡å­—ï¼‰
            
        Returns:
            æ•´å½¢ã•ã‚ŒãŸã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚µãƒãƒªãƒ¼ã®æ–‡å­—åˆ—
        """
        summary = self.get_invocations_summary()
        
        if summary['total_invocations'] == 0:
            return ""
        
        lines = []
        lines.append("=" * width)
        lines.append("ğŸ“ˆ SESSION SUMMARY:")
        lines.append("=" * width)
        lines.append(f"Total LLM Calls: {summary['total_invocations']}")
        lines.append(f"Total Tokens: {summary['total_tokens']} ({summary['total_input_tokens']} input + {summary['total_output_tokens']} output)")
        if summary['total_cached_tokens'] > 0:
            lines.append(f"ğŸ’¾ Total Cached: {summary['total_cached_tokens']} tokens")
        lines.append(f"ğŸ’° Total Cost: ${summary['total_cost_usd']:.6f}")
        lines.append(f"ğŸ“Š Average: {summary['average_tokens_per_invocation']:.1f} tokens/call, ${summary['average_cost_per_invocation']:.6f}/call")
        lines.append("=" * width)
        
        return "\n".join(lines)
    
    @contextmanager
    def track_query(self):
        """
        1ã¤ã®ã‚¯ã‚¨ãƒªï¼ˆå‡¦ç†å˜ä½ï¼‰ã‚’è¿½è·¡ã™ã‚‹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼
        
        ä½¿ã„æ–¹:
            with token_counter.track_query() as query:
                # ainvokeå®Ÿè¡Œ
                response = await agent.ainvoke(...)
                # ã‚¯ã‚¨ãƒªãƒ¬ãƒãƒ¼ãƒˆè¡¨ç¤º
                print(query.report())
        """
        start_index = len(self.invocation_history)
        
        class QueryTracker:
            def __init__(self, counter, start_idx):
                self.counter = counter
                self.start_index = start_idx
            
            def report(self, width: int = 70) -> str:
                """ã“ã®ã‚¯ã‚¨ãƒªã®ãƒ¬ãƒãƒ¼ãƒˆã‚’è¿”ã™"""
                return self.counter.format_loop_report(self.start_index, width)
        
        yield QueryTracker(self, start_index)
    
    # ===== ã‚°ãƒ­ãƒ¼ãƒãƒ«çµ±è¨ˆæ©Ÿèƒ½ =====
    
    def save_session_to_global(self, session_label: Optional[str] = None) -> None:
        """
        ç¾åœ¨ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³çµ±è¨ˆã‚’ã‚°ãƒ­ãƒ¼ãƒãƒ«å±¥æ­´ã«ä¿å­˜
        
        Args:
            session_label: ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ãƒ©ãƒ™ãƒ«ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰ã€‚çœç•¥æ™‚ã¯è‡ªå‹•ç”Ÿæˆ
        """
        if not self.invocation_history:
            return  # ç©ºã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã¯ä¿å­˜ã—ãªã„
        
        summary = self.get_invocations_summary()
        
        session_record = {
            "session_label": session_label or f"Session {len(self._global_history) + 1}",
            "timestamp": __import__('datetime').datetime.now().isoformat(),
            "total_invocations": summary["total_invocations"],
            "total_input_tokens": summary["total_input_tokens"],
            "total_cached_tokens": summary["total_cached_tokens"],
            "total_output_tokens": summary["total_output_tokens"],
            "total_tokens": summary["total_tokens"],
            "total_cost_usd": summary["total_cost_usd"],
            "invocations": self.invocation_history.copy(),  # è©³ç´°ã‚‚ä¿å­˜
        }
        
        self._global_history.append(session_record)
    
    @classmethod
    def get_global_history(cls) -> List[Dict[str, Any]]:
        """
        å…¨ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ã‚°ãƒ­ãƒ¼ãƒãƒ«å±¥æ­´ã‚’å–å¾—
        
        Returns:
            List of session records
        """
        return cls._global_history.copy()
    
    @classmethod
    def get_global_summary(cls) -> Dict[str, Any]:
        """
        å…¨ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’é›†è¨ˆã—ãŸã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚µãƒãƒªãƒ¼ã‚’å–å¾—
        
        Returns:
            Summary of all sessions combined
        """
        if not cls._global_history:
            return {
                "total_sessions": 0,
                "total_invocations": 0,
                "total_input_tokens": 0,
                "total_cached_tokens": 0,
                "total_output_tokens": 0,
                "total_tokens": 0,
                "total_cost_usd": 0.0,
            }
        
        total_sessions = len(cls._global_history)
        total_invocations = sum(s["total_invocations"] for s in cls._global_history)
        total_input = sum(s["total_input_tokens"] for s in cls._global_history)
        total_cached = sum(s["total_cached_tokens"] for s in cls._global_history)
        total_output = sum(s["total_output_tokens"] for s in cls._global_history)
        total_cost = sum(s["total_cost_usd"] for s in cls._global_history)
        
        return {
            "total_sessions": total_sessions,
            "total_invocations": total_invocations,
            "total_input_tokens": total_input,
            "total_cached_tokens": total_cached,
            "total_output_tokens": total_output,
            "total_tokens": total_input + total_output,
            "total_cost_usd": round(total_cost, 6),
        }
    
    @classmethod
    def format_global_summary(cls, width: int = 70) -> str:
        """
        å…¨ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚µãƒãƒªãƒ¼ã‚’æ•´å½¢ã—ã¦è¿”ã™
        
        Args:
            width: è¡¨ç¤ºå¹…ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 70æ–‡å­—ï¼‰
            
        Returns:
            æ•´å½¢ã•ã‚ŒãŸã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚µãƒãƒªãƒ¼ã®æ–‡å­—åˆ—
        """
        summary = cls.get_global_summary()
        
        if summary["total_sessions"] == 0:
            return ""
        
        lines = []
        lines.append("=" * width)
        lines.append("ğŸŒ GLOBAL SUMMARY (All Sessions):")
        lines.append("=" * width)
        lines.append(f"Total Sessions: {summary['total_sessions']}")
        lines.append(f"Total LLM Calls: {summary['total_invocations']}")
        lines.append(f"Total Tokens: {summary['total_tokens']} ({summary['total_input_tokens']} input + {summary['total_output_tokens']} output)")
        if summary['total_cached_tokens'] > 0:
            lines.append(f"ğŸ’¾ Total Cached: {summary['total_cached_tokens']} tokens")
        lines.append(f"ğŸ’° Total Cost: ${summary['total_cost_usd']:.6f}")
        lines.append("=" * width)
        
        return "\n".join(lines)
    
    @classmethod
    def format_global_detailed(cls, width: int = 70) -> str:
        """
        å„ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®è©³ç´°ã‚’å«ã‚€ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ¬ãƒãƒ¼ãƒˆã‚’æ•´å½¢ã—ã¦è¿”ã™
        
        Args:
            width: è¡¨ç¤ºå¹…ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 70æ–‡å­—ï¼‰
            
        Returns:
            æ•´å½¢ã•ã‚ŒãŸè©³ç´°ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ¬ãƒãƒ¼ãƒˆã®æ–‡å­—åˆ—
        """
        if not cls._global_history:
            return ""
        
        lines = []
        lines.append("=" * width)
        lines.append("ğŸŒ GLOBAL DETAILED REPORT:")
        lines.append("=" * width)
        
        for i, session in enumerate(cls._global_history, 1):
            lines.append(f"\nğŸ“¦ {session['session_label']}")
            lines.append(f"   Time: {session['timestamp']}")
            lines.append(f"   Calls: {session['total_invocations']}")
            lines.append(f"   Tokens: {session['total_tokens']} ({session['total_input_tokens']} input + {session['total_output_tokens']} output)")
            if session['total_cached_tokens'] > 0:
                lines.append(f"   ğŸ’¾ Cached: {session['total_cached_tokens']} tokens")
            lines.append(f"   ğŸ’° Cost: ${session['total_cost_usd']:.6f}")
        
        lines.append("\n" + "-" * width)
        summary = cls.get_global_summary()
        lines.append(f"ğŸŒ Total: {summary['total_sessions']} sessions, {summary['total_invocations']} calls, {summary['total_tokens']} tokens, ${summary['total_cost_usd']:.6f}")
        lines.append("=" * width)
        
        return "\n".join(lines)
    
    @classmethod
    def reset_global_history(cls) -> None:
        """
        ã‚°ãƒ­ãƒ¼ãƒãƒ«å±¥æ­´ã‚’ã‚¯ãƒªã‚¢
        
        è­¦å‘Š: å…¨ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ç´¯ç©çµ±è¨ˆãŒå‰Šé™¤ã•ã‚Œã¾ã™
        """
        cls._global_history.clear()






# Convenience functions for cost calculation
# è²»ç”¨è¨ˆç®—ã®ãŸã‚ã®ä¾¿åˆ©é–¢æ•°

def calculate_openai_cost(model: str, input_tokens: int, output_tokens: int) -> Dict[str, float]:
    """
    Calculate OpenAI API cost for given token usage
    æŒ‡å®šã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã«å¯¾ã™ã‚‹OpenAI APIã®è²»ç”¨ã‚’è¨ˆç®—
    """
    return OpenAIPricingCalculator.calculate_cost(model, input_tokens, output_tokens)


