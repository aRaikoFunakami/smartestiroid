"""
ãƒ†ã‚¹ãƒˆå¤±æ•—ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«

JSONLãƒ­ã‚°ã‹ã‚‰å¤±æ•—ãƒ†ã‚¹ãƒˆã‚’æŠ½å‡ºã—ã€LLMã‚’ä½¿ç”¨ã—ã¦åˆ†æã€
Markdownãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¾ã™ã€‚

ä½¿ç”¨ä¾‹:
    from smartestiroid.utils.failure_report_generator import FailureReportGenerator
    
    generator = FailureReportGenerator(
        log_dir=Path("smartestiroid_logs/run_20251205_194626"),
        use_llm=True
    )
    report_path = generator.generate_report()
"""

import json
import re
from pathlib import Path
from typing import List, Dict, Any, Optional, Literal
from dataclasses import dataclass, field
from datetime import datetime

from pydantic import BaseModel, Field

# LangChainã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒŠãƒ«ï¼‰
try:
    from langchain_openai import ChatOpenAI
    LANGCHAIN_AVAILABLE = True
except ImportError:
    LANGCHAIN_AVAILABLE = False


# ========================================
# Pydantic ãƒ¢ãƒ‡ãƒ«ï¼ˆLLM Structured Outputç”¨ï¼‰
# ========================================

class FailureAnalysis(BaseModel):
    """LLMãŒå‡ºåŠ›ã™ã‚‹å¤±æ•—åˆ†æï¼ˆå½¢å¼å›ºå®šï¼‰"""
    
    failure_category: Literal[
        "APPIUM_CONNECTION_ERROR",    # Appiumæ¥ç¶šã‚¨ãƒ©ãƒ¼
        "ELEMENT_NOT_FOUND",          # è¦ç´ ãŒè¦‹ã¤ã‹ã‚‰ãªã„
        "VERIFICATION_FAILED",        # æ¤œè¨¼å¤±æ•—ï¼ˆLLMåˆ¤å®šï¼‰
        "TIMEOUT",                    # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
        "LLM_JUDGMENT_ERROR",         # LLMåˆ¤å®šãƒŸã‚¹
        "APP_CRASH",                  # ã‚¢ãƒ—ãƒªã‚¯ãƒ©ãƒƒã‚·ãƒ¥
        "SESSION_ERROR",              # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¨ãƒ©ãƒ¼
        "UNKNOWN"                     # ä¸æ˜
    ] = Field(description="å¤±æ•—ã‚«ãƒ†ã‚´ãƒª")
    
    summary: str = Field(description="å¤±æ•—æ¦‚è¦ï¼ˆ1æ–‡ã§ç°¡æ½”ã«ï¼‰")
    
    root_causes: List[str] = Field(
        description="æŠ€è¡“çš„ãªåŸå› ï¼ˆ1-3å€‹ï¼‰",
        min_length=1,
        max_length=3
    )
    
    recommendations: List[str] = Field(
        description="å…·ä½“çš„ãªå¯¾å‡¦æ³•ï¼ˆå„ªå…ˆåº¦é †ã€1-3å€‹ï¼‰",
        min_length=1,
        max_length=3
    )
    
    confidence: Literal["HIGH", "MEDIUM", "LOW"] = Field(
        description="åˆ†æã®ç¢ºä¿¡åº¦"
    )


# ã‚«ãƒ†ã‚´ãƒªè¡¨ç¤ºå
CATEGORY_DISPLAY = {
    "APPIUM_CONNECTION_ERROR": "ğŸ”Œ Appiumæ¥ç¶šã‚¨ãƒ©ãƒ¼",
    "ELEMENT_NOT_FOUND": "ğŸ” è¦ç´ ãŒè¦‹ã¤ã‹ã‚‰ãªã„",
    "VERIFICATION_FAILED": "âŒ æ¤œè¨¼å¤±æ•—",
    "TIMEOUT": "â±ï¸ ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ",
    "LLM_JUDGMENT_ERROR": "ğŸ¤– LLMåˆ¤å®šãƒŸã‚¹",
    "APP_CRASH": "ğŸ’¥ ã‚¢ãƒ—ãƒªã‚¯ãƒ©ãƒƒã‚·ãƒ¥",
    "SESSION_ERROR": "ğŸ”— ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¨ãƒ©ãƒ¼",
    "UNKNOWN": "â“ ä¸æ˜",
}


# ========================================
# ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹
# ========================================

@dataclass
class FailedTestInfo:
    """å¤±æ•—ã—ãŸãƒ†ã‚¹ãƒˆã®æƒ…å ±"""
    test_id: str
    title: str
    steps: str
    expected: str
    
    # å¤±æ•—æƒ…å ±
    failed_step: Optional[str] = None
    error_message: Optional[str] = None
    error_type: Optional[str] = None
    failure_timestamp: Optional[str] = None
    
    # LLMæ¤œè¨¼æƒ…å ±
    verification_phase1: Optional[Dict[str, Any]] = None
    verification_phase2: Optional[Dict[str, Any]] = None
    
    # ç”»é¢æƒ…å ±
    last_screen_type: Optional[str] = None
    last_screen_xml: Optional[str] = None
    screenshots: List[Dict[str, str]] = field(default_factory=list)
    
    # é€²æ—æƒ…å ±
    progress_summary: Optional[str] = None
    completed_steps: List[str] = field(default_factory=list)
    
    # ãƒ­ã‚°è¡Œç¯„å›²
    log_start_line: int = 0
    log_end_line: int = 0
    
    # LLMåˆ†æçµæœ
    analysis: Optional[FailureAnalysis] = None


# ========================================
# ãƒ¡ã‚¤ãƒ³ã‚¯ãƒ©ã‚¹
# ========================================

class FailureReportGenerator:
    """ãƒ†ã‚¹ãƒˆå¤±æ•—ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå™¨"""
    
    def __init__(
        self,
        log_dir: Path,
        use_llm: bool = True,
        model_name: str = "gpt-4.1-mini"
    ):
        """
        Args:
            log_dir: ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆrun_YYYYMMDD_HHMMSSï¼‰
            use_llm: LLMã‚’ä½¿ç”¨ã—ã¦åˆ†æã™ã‚‹ã‹
            model_name: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«å
        """
        self.log_dir = Path(log_dir)
        self.use_llm = use_llm and LANGCHAIN_AVAILABLE
        self.model_name = model_name
        
        # JSONLãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™
        jsonl_files = list(self.log_dir.glob("*.jsonl"))
        if not jsonl_files:
            raise FileNotFoundError(f"JSONLãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.log_dir}")
        self.log_file = jsonl_files[0]
        
        # ãƒ­ã‚°ã‚’ãƒ­ãƒ¼ãƒ‰
        self.entries: List[Dict[str, Any]] = []
        self._load_log()
        
        # å¤±æ•—ãƒ†ã‚¹ãƒˆã‚’æŠ½å‡º
        self.failed_tests: List[FailedTestInfo] = []
        self._extract_failed_tests()
    
    def _load_log(self):
        """JSONLãƒ­ã‚°ã‚’èª­ã¿è¾¼ã‚€"""
        with open(self.log_file, "r", encoding="utf-8") as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if line:
                    try:
                        entry = json.loads(line)
                        entry["_line_num"] = line_num
                        self.entries.append(entry)
                    except json.JSONDecodeError:
                        pass
    
    def _extract_failed_tests(self):
        """å¤±æ•—ã—ãŸãƒ†ã‚¹ãƒˆã‚’æŠ½å‡º"""
        current_test: Optional[FailedTestInfo] = None
        current_test_start_line = 0
        
        for entry in self.entries:
            cat = entry.get("cat", "")
            evt = entry.get("evt", "")
            data = entry.get("data", {}) or {}
            line_num = entry.get("_line_num", 0)
            
            # ãƒ†ã‚¹ãƒˆé–‹å§‹
            if cat == "TEST" and evt == "START" and "test_id" in data:
                if current_test and current_test.error_message:
                    # å‰ã®ãƒ†ã‚¹ãƒˆãŒå¤±æ•—ã—ã¦ã„ãŸã‚‰ä¿å­˜
                    current_test.log_end_line = line_num - 1
                    self.failed_tests.append(current_test)
                
                current_test = FailedTestInfo(
                    test_id=data.get("test_id", ""),
                    title=data.get("title", ""),
                    steps=data.get("steps", ""),
                    expected=data.get("expected", ""),
                    log_start_line=line_num
                )
                current_test_start_line = line_num
            
            if current_test is None:
                continue
            
            # ç”»é¢ã‚¿ã‚¤ãƒ—
            if cat == "SCREEN" and evt == "COMPLETE":
                current_test.last_screen_type = data.get("screen_type")
            
            # ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆ
            if cat == "SCREEN" and evt == "UPDATE" and "image_path" in data:
                current_test.screenshots.append({
                    "path": data.get("image_path", ""),
                    "filename": data.get("image_filename", ""),
                    "label": data.get("label"),
                    "timestamp": entry.get("ts", "")
                })
            
            # XMLï¼ˆLLMãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‹ã‚‰æŠ½å‡ºï¼‰
            if cat == "LLM" and evt == "START":
                user_prompt = data.get("user_prompt", "")
                if "<hierarchy" in user_prompt:
                    # XMLã‚’æŠ½å‡º
                    match = re.search(r'(<hierarchy.*?</hierarchy>)', user_prompt, re.DOTALL)
                    if match:
                        current_test.last_screen_xml = match.group(1)
            
            # æ¤œè¨¼çµæœ
            if cat == "LLM" and evt == "VERIFY_RESPONSE":
                phase = data.get("phase")
                if phase == 1:
                    current_test.verification_phase1 = data
                elif phase == 2:
                    current_test.verification_phase2 = data
            
            # é€²æ—ã‚µãƒãƒªãƒ¼
            if cat == "OBJECTIVE" and evt == "UPDATE":
                summary = data.get("summary")
                if summary:
                    current_test.progress_summary = summary
            
            # ã‚¹ãƒ†ãƒƒãƒ—å®Œäº†
            if cat == "STEP" and evt == "COMPLETE" and data.get("success"):
                step = data.get("step", "")
                if step:
                    current_test.completed_steps.append(step)
            
            # ã‚¹ãƒ†ãƒƒãƒ—å¤±æ•—
            if cat == "STEP" and evt == "FAIL":
                current_test.failed_step = data.get("step", "")
                error = data.get("error", "")
                current_test.error_message = error
                
                # ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—ã‚’æŠ½å‡º
                if "InvalidContextError" in error:
                    current_test.error_type = "InvalidContextError"
                elif "TimeoutError" in error or "timeout" in error.lower():
                    current_test.error_type = "TimeoutError"
                elif "NoSuchElement" in error:
                    current_test.error_type = "NoSuchElementError"
                else:
                    current_test.error_type = "UnknownError"
            
            # ãƒ†ã‚¹ãƒˆå¤±æ•—
            if cat == "TEST" and evt == "FAIL":
                current_test.failure_timestamp = entry.get("ts", "")
                if not current_test.error_message:
                    current_test.error_message = data.get("error", entry.get("msg", ""))
            
            # ãƒ†ã‚¹ãƒˆçµ‚äº†ï¼ˆæ¬¡ã®ãƒ†ã‚¹ãƒˆãŒå§‹ã¾ã‚‹ã‹ã€ã‚»ãƒƒã‚·ãƒ§ãƒ³çµ‚äº†ï¼‰
            if cat == "SESSION" and evt == "END":
                if current_test and current_test.error_message:
                    current_test.log_end_line = line_num
                    self.failed_tests.append(current_test)
                    current_test = None
        
        # æœ€å¾Œã®ãƒ†ã‚¹ãƒˆ
        if current_test and current_test.error_message:
            current_test.log_end_line = len(self.entries)
            self.failed_tests.append(current_test)
    
    def _analyze_with_llm(self, test_info: FailedTestInfo) -> Optional[FailureAnalysis]:
        """LLMã‚’ä½¿ç”¨ã—ã¦å¤±æ•—ã‚’åˆ†æ"""
        if not self.use_llm:
            return None
        
        try:
            from langchain_openai import ChatOpenAI
            
            llm = ChatOpenAI(
                model=self.model_name,
                temperature=0,
                timeout=30,
                max_retries=2
            )
            
            # Structured Outputã‚’ä½¿ç”¨
            structured_llm = llm.with_structured_output(FailureAnalysis)
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
            prompt = self._build_analysis_prompt(test_info)
            
            # LLMå‘¼ã³å‡ºã—
            result = structured_llm.invoke(prompt)
            return result
            
        except Exception as e:
            print(f"âš ï¸ LLMåˆ†æã‚¨ãƒ©ãƒ¼ ({test_info.test_id}): {e}")
            return None
    
    def _build_analysis_prompt(self, test_info: FailedTestInfo) -> str:
        """åˆ†æç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰"""
        prompt = f"""ã‚ãªãŸã¯ãƒ¢ãƒã‚¤ãƒ«ã‚¢ãƒ—ãƒªãƒ†ã‚¹ãƒˆè‡ªå‹•åŒ–ã®å°‚é–€å®¶ã§ã™ã€‚
ä»¥ä¸‹ã®ãƒ†ã‚¹ãƒˆå¤±æ•—ã‚’åˆ†æã—ã€æ§‹é€ åŒ–ã•ã‚ŒãŸåˆ†æçµæœã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚

## ãƒ†ã‚¹ãƒˆæƒ…å ±
- **ãƒ†ã‚¹ãƒˆID**: {test_info.test_id}
- **ãƒ†ã‚¹ãƒˆå**: {test_info.title}
- **ãƒ†ã‚¹ãƒˆæ‰‹é †**:
{test_info.steps}
- **æœŸå¾…çµæœ**: {test_info.expected}

## é€²æ—çŠ¶æ³
- å®Œäº†ã‚¹ãƒ†ãƒƒãƒ—: {len(test_info.completed_steps)}
- å¤±æ•—ã—ãŸã‚¹ãƒ†ãƒƒãƒ—: {test_info.failed_step or "ä¸æ˜"}
"""
        
        if test_info.progress_summary:
            prompt += f"\n### é€²æ—ã‚µãƒãƒªãƒ¼\n{test_info.progress_summary}\n"
        
        if test_info.last_screen_type:
            prompt += f"\n## ç›´å‰ã®ç”»é¢çŠ¶æ…‹\n- ç”»é¢ã‚¿ã‚¤ãƒ—: {test_info.last_screen_type}\n"
        
        prompt += f"""
## ã‚¨ãƒ©ãƒ¼æƒ…å ±
- **ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—**: {test_info.error_type or "ä¸æ˜"}
- **ã‚¨ãƒ©ãƒ¼å†…å®¹**: {test_info.error_message[:500] if test_info.error_message else "ä¸æ˜"}
"""
        
        if test_info.verification_phase1:
            prompt += f"""
## LLMæ¤œè¨¼çµæœï¼ˆPhase 1ï¼‰
- success: {test_info.verification_phase1.get("success")}
- reason: {test_info.verification_phase1.get("reason", "")[:300]}
"""
        
        if test_info.verification_phase2:
            prompt += f"""
## LLMæ¤œè¨¼çµæœï¼ˆPhase 2ï¼‰
- verified: {test_info.verification_phase2.get("verified")}
- confidence: {test_info.verification_phase2.get("confidence")}
- reason: {test_info.verification_phase2.get("reason", "")[:300]}
- discrepancy: {test_info.verification_phase2.get("discrepancy")}
"""
        
        prompt += """
## åˆ†æã®è¦³ç‚¹
1. **failure_category**: æœ€ã‚‚è©²å½“ã™ã‚‹ã‚«ãƒ†ã‚´ãƒªã‚’1ã¤é¸æŠ
   - APPIUM_CONNECTION_ERROR: Appiumã‚µãƒ¼ãƒãƒ¼ã¨ã®æ¥ç¶šå•é¡Œ
   - ELEMENT_NOT_FOUND: ç”»é¢è¦ç´ ãŒè¦‹ã¤ã‹ã‚‰ãªã„
   - VERIFICATION_FAILED: LLMã«ã‚ˆã‚‹ç”»é¢æ¤œè¨¼ãŒå¤±æ•—
   - TIMEOUT: æ“ä½œã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
   - LLM_JUDGMENT_ERROR: LLMã®åˆ¤æ–­ãƒŸã‚¹
   - APP_CRASH: ã‚¢ãƒ—ãƒªã®ã‚¯ãƒ©ãƒƒã‚·ãƒ¥
   - SESSION_ERROR: Appiumã‚»ãƒƒã‚·ãƒ§ãƒ³ã®å•é¡Œ
   - UNKNOWN: ä¸Šè¨˜ã«è©²å½“ã—ãªã„

2. **summary**: ä½•ãŒèµ·ããŸã‹ã‚’1æ–‡ã§ï¼ˆæŠ€è¡“ç”¨èªã‚’ä½¿ã‚ãšç°¡æ½”ã«ï¼‰

3. **root_causes**: æŠ€è¡“çš„ãªåŸå› ï¼ˆ1-3å€‹ã€ç®‡æ¡æ›¸ãç”¨ï¼‰

4. **recommendations**: å…·ä½“çš„ãªå¯¾å‡¦æ³•ï¼ˆå„ªå…ˆåº¦é †ã€1-3å€‹ã€å®Ÿè¡Œå¯èƒ½ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ï¼‰

5. **confidence**: åˆ†æã®ç¢ºä¿¡åº¦
   - HIGH: ãƒ­ã‚°ã‹ã‚‰åŸå› ãŒæ˜ç¢ºã«ç‰¹å®šã§ãã‚‹
   - MEDIUM: åŸå› ã¯æ¨å®šã§ãã‚‹ãŒç¢ºå®šã§ã¯ãªã„
   - LOW: æƒ…å ±ãŒä¸è¶³ã—ã¦ãŠã‚Šæ¨æ¸¬ã®è¦ç´ ãŒå¤§ãã„

## é‡è¦
- æ¨æ¸¬ã¯é¿ã‘ã€ãƒ­ã‚°ã‹ã‚‰èª­ã¿å–ã‚Œã‚‹äº‹å®Ÿã«åŸºã¥ãã“ã¨
- ãƒªã‚³ãƒ¡ãƒ³ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã¯å®Ÿè¡Œå¯èƒ½ãªå…·ä½“çš„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã«ã™ã‚‹ã“ã¨
"""
        return prompt
    
    def _fallback_analysis(self, test_info: FailedTestInfo) -> FailureAnalysis:
        """LLMã‚’ä½¿ç”¨ã—ãªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯åˆ†æ"""
        error = test_info.error_message or ""
        
        # ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã«åŸºã¥ãåˆ†é¡
        if "cannot be proxied" in error or "instrumentation process" in error:
            return FailureAnalysis(
                failure_category="APPIUM_CONNECTION_ERROR",
                summary="Appiumã‚µãƒ¼ãƒãƒ¼ã¨ã®é€šä¿¡ãŒæ–­çµ¶ã—ã¾ã—ãŸ",
                root_causes=[
                    "UiAutomator2ã®instrumentationãƒ—ãƒ­ã‚»ã‚¹ãŒã‚¯ãƒ©ãƒƒã‚·ãƒ¥",
                    "Androidç«¯æœ«ã¨ã®æ¥ç¶šãŒä¸å®‰å®š"
                ],
                recommendations=[
                    "Androidç«¯æœ«/ã‚¨ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ã‚’å†èµ·å‹•ã™ã‚‹",
                    "Appiumã‚µãƒ¼ãƒãƒ¼ã‚’å†èµ·å‹•ã™ã‚‹",
                    "adb devicesã§æ¥ç¶šçŠ¶æ…‹ã‚’ç¢ºèªã™ã‚‹"
                ],
                confidence="HIGH"
            )
        elif "NoSuchElement" in error or "not found" in error.lower():
            return FailureAnalysis(
                failure_category="ELEMENT_NOT_FOUND",
                summary="ç”»é¢ä¸Šã§æŒ‡å®šã—ãŸè¦ç´ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ",
                root_causes=[
                    "è¦ç´ ã®ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ãŒæ­£ã—ããªã„",
                    "ç”»é¢é·ç§»ãŒå®Œäº†ã—ã¦ã„ãªã„",
                    "è¦ç´ ãŒç”»é¢å¤–ã«ã‚ã‚‹"
                ],
                recommendations=[
                    "è¦ç´ ã®XPathã‚„resource-idã‚’ç¢ºèªã™ã‚‹",
                    "å¾…æ©Ÿæ™‚é–“ã‚’å¢—ã‚„ã™",
                    "ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã§è¦ç´ ã‚’è¡¨ç¤ºã™ã‚‹"
                ],
                confidence="MEDIUM"
            )
        elif "timeout" in error.lower():
            return FailureAnalysis(
                failure_category="TIMEOUT",
                summary="æ“ä½œãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸ",
                root_causes=["å‡¦ç†ã«æ™‚é–“ãŒã‹ã‹ã‚Šã™ããŸ"],
                recommendations=["ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå€¤ã‚’å¢—ã‚„ã™"],
                confidence="MEDIUM"
            )
        else:
            return FailureAnalysis(
                failure_category="UNKNOWN",
                summary="ãƒ†ã‚¹ãƒˆãŒå¤±æ•—ã—ã¾ã—ãŸï¼ˆè©³ç´°ã¯ãƒ­ã‚°ã‚’ç¢ºèªï¼‰",
                root_causes=["è©³ç´°ãªãƒ­ã‚°ç¢ºèªãŒå¿…è¦"],
                recommendations=["è©³ç´°ãƒ­ã‚°ã‚’ç¢ºèªã—ã¦åŸå› ã‚’ç‰¹å®šã™ã‚‹"],
                confidence="LOW"
            )
    
    def generate_report(self) -> Path:
        """Markdownãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        if not self.failed_tests:
            print("âœ… å¤±æ•—ã—ãŸãƒ†ã‚¹ãƒˆã¯ã‚ã‚Šã¾ã›ã‚“")
            return self._generate_empty_report()
        
        # LLMåˆ†æã‚’å®Ÿè¡Œ
        if self.use_llm:
            print(f"ğŸ¤– LLMåˆ†æã‚’å®Ÿè¡Œä¸­... ({len(self.failed_tests)}ä»¶)")
            for i, test_info in enumerate(self.failed_tests, 1):
                print(f"  [{i}/{len(self.failed_tests)}] {test_info.test_id}...")
                test_info.analysis = self._analyze_with_llm(test_info)
                if test_info.analysis is None:
                    test_info.analysis = self._fallback_analysis(test_info)
        else:
            for test_info in self.failed_tests:
                test_info.analysis = self._fallback_analysis(test_info)
        
        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        report_content = self._build_report()
        
        # ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›
        report_path = self.log_dir / "failure_report.md"
        with open(report_path, "w", encoding="utf-8") as f:
            f.write(report_content)
        
        print(f"âœ… ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¾ã—ãŸ: {report_path}")
        return report_path
    
    def _generate_empty_report(self) -> Path:
        """å¤±æ•—ãƒ†ã‚¹ãƒˆãŒãªã„å ´åˆã®ãƒ¬ãƒãƒ¼ãƒˆ"""
        content = f"""# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œãƒ¬ãƒãƒ¼ãƒˆ

**å®Ÿè¡Œæ—¥æ™‚**: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}  
**ãƒ­ã‚°ãƒ•ã‚©ãƒ«ãƒ€**: `{self.log_dir.name}`

---

## çµæœã‚µãƒãƒªãƒ¼

âœ… **ã™ã¹ã¦ã®ãƒ†ã‚¹ãƒˆãŒæˆåŠŸã—ã¾ã—ãŸ**

å¤±æ•—ã—ãŸãƒ†ã‚¹ãƒˆã¯ã‚ã‚Šã¾ã›ã‚“ã€‚
"""
        report_path = self.log_dir / "failure_report.md"
        with open(report_path, "w", encoding="utf-8") as f:
            f.write(content)
        return report_path
    
    def _build_report(self) -> str:
        """ãƒ¬ãƒãƒ¼ãƒˆæœ¬ä½“ã‚’æ§‹ç¯‰"""
        lines = []
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼
        run_time = self.log_dir.name.replace("run_", "").replace("_", " ")
        lines.append("# ãƒ†ã‚¹ãƒˆå¤±æ•—ãƒ¬ãƒãƒ¼ãƒˆ")
        lines.append("")
        lines.append(f"**å®Ÿè¡Œæ—¥æ™‚**: {run_time}  ")
        lines.append(f"**ãƒ­ã‚°ãƒ•ã‚©ãƒ«ãƒ€**: `{self.log_dir.name}`  ")
        lines.append(f"**åˆ†æãƒ¢ãƒ¼ãƒ‰**: {'LLMåˆ†æ' if self.use_llm else 'ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°'}")
        lines.append("")
        lines.append("---")
        lines.append("")
        
        # ã‚µãƒãƒªãƒ¼
        lines.append("## ã‚µãƒãƒªãƒ¼")
        lines.append("")
        lines.append("| é …ç›® | å€¤ |")
        lines.append("|------|-----|")
        lines.append(f"| å¤±æ•—ãƒ†ã‚¹ãƒˆæ•° | {len(self.failed_tests)} |")
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥é›†è¨ˆ
        category_counts: Dict[str, int] = {}
        for test in self.failed_tests:
            if test.analysis:
                cat = test.analysis.failure_category
                category_counts[cat] = category_counts.get(cat, 0) + 1
        
        for cat, count in category_counts.items():
            display = CATEGORY_DISPLAY.get(cat, cat)
            lines.append(f"| {display} | {count} |")
        
        lines.append("")
        lines.append("---")
        lines.append("")
        
        # å„å¤±æ•—ãƒ†ã‚¹ãƒˆã®è©³ç´°
        lines.append("## å¤±æ•—ãƒ†ã‚¹ãƒˆä¸€è¦§")
        lines.append("")
        
        for test_info in self.failed_tests:
            lines.extend(self._build_test_section(test_info))
            lines.append("")
        
        return "\n".join(lines)
    
    def _build_test_section(self, test_info: FailedTestInfo) -> List[str]:
        """å„ãƒ†ã‚¹ãƒˆã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ§‹ç¯‰"""
        lines = []
        analysis = test_info.analysis
        
        # ãƒ†ã‚¹ãƒˆãƒ˜ãƒƒãƒ€ãƒ¼
        lines.append(f"### {test_info.test_id}: {test_info.title}")
        lines.append("")
        
        # åŸºæœ¬æƒ…å ±ãƒ†ãƒ¼ãƒ–ãƒ«
        lines.append("| é …ç›® | å†…å®¹ |")
        lines.append("|------|------|")
        
        if analysis:
            category_display = CATEGORY_DISPLAY.get(analysis.failure_category, analysis.failure_category)
            lines.append(f"| **çµæœ** | {category_display} |")
        
        if test_info.failure_timestamp:
            time_only = test_info.failure_timestamp[11:19] if len(test_info.failure_timestamp) >= 19 else test_info.failure_timestamp
            lines.append(f"| **å¤±æ•—æ™‚åˆ»** | {time_only} |")
        
        if test_info.failed_step:
            step_display = test_info.failed_step[:50] + "..." if len(test_info.failed_step) > 50 else test_info.failed_step
            lines.append(f"| **å¤±æ•—ã‚¹ãƒ†ãƒƒãƒ—** | {step_display} |")
        
        if analysis:
            lines.append(f"| **ä¿¡é ¼åº¦** | {analysis.confidence} |")
        
        lines.append("")
        
        # å¤±æ•—æ¦‚è¦
        if analysis:
            lines.append("#### å¤±æ•—æ¦‚è¦")
            lines.append("")
            lines.append(analysis.summary)
            lines.append("")
        
        # åŸå› è©³ç´°
        if analysis and analysis.root_causes:
            lines.append("#### åŸå› è©³ç´°")
            lines.append("")
            for cause in analysis.root_causes:
                lines.append(f"- {cause}")
            lines.append("")
        
        # æ¨å¥¨å¯¾å¿œ
        if analysis and analysis.recommendations:
            lines.append("#### æ¨å¥¨å¯¾å¿œ")
            lines.append("")
            for i, rec in enumerate(analysis.recommendations, 1):
                lines.append(f"{i}. {rec}")
            lines.append("")
        
        # å‚ç…§ãƒ­ã‚°
        lines.append("#### å‚ç…§ãƒ­ã‚°")
        lines.append("")
        log_filename = self.log_file.name
        lines.append(f"- [è©³ç´°ãƒ­ã‚°](./{log_filename}) (è¡Œ{test_info.log_start_line}-{test_info.log_end_line})")
        
        # ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆ
        if test_info.screenshots:
            last_screenshots = test_info.screenshots[-3:]  # æœ€å¾Œã®3æš
            images_dir = f"{self.log_file.stem}_images"
            for ss in last_screenshots:
                filename = ss.get("filename", "")
                label = ss.get("label") or "Screenshot"
                lines.append(f"- [{label}](./{images_dir}/{filename})")
        
        lines.append("")
        lines.append("---")
        
        return lines


# ========================================
# ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
# ========================================

def main():
    """ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã‚¨ãƒ³ãƒˆãƒªãƒã‚¤ãƒ³ãƒˆ"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description="ãƒ†ã‚¹ãƒˆå¤±æ•—ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆãƒ„ãƒ¼ãƒ«"
    )
    parser.add_argument("log_dir", help="ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆrun_YYYYMMDD_HHMMSSï¼‰")
    parser.add_argument("--no-llm", action="store_true", help="LLMåˆ†æã‚’ä½¿ç”¨ã—ãªã„")
    parser.add_argument("--model", default="gpt-4.1-mini", help="ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«")
    
    args = parser.parse_args()
    
    try:
        generator = FailureReportGenerator(
            log_dir=Path(args.log_dir),
            use_llm=not args.no_llm,
            model_name=args.model
        )
        
        report_path = generator.generate_report()
        print(f"ğŸ”— file://{report_path.absolute()}")
        
    except FileNotFoundError as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        return 1
    except Exception as e:
        print(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())
