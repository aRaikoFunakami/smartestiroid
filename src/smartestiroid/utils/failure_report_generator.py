"""
ãƒ†ã‚¹ãƒˆå¤±æ•—ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«

JSONLãƒ­ã‚°ã‹ã‚‰å¤±æ•—ãƒ†ã‚¹ãƒˆã‚’æŠ½å‡ºã—ã€LLMã‚’ä½¿ç”¨ã—ã¦åˆ†æã€
Markdownãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¾ã™ã€‚

ä½¿ç”¨ä¾‹:
    from smartestiroid.utils.failure_report_generator import FailureReportGenerator
    
    generator = FailureReportGenerator(
        log_dir=Path("smartestiroid_logs/run_20251205_194626"),
        use_llm=True
    )
    report_path = generator.generate_report()
"""

import json
import re
from pathlib import Path
from typing import List, Dict, Any, Optional, Literal
from dataclasses import dataclass, field
from datetime import datetime

from pydantic import BaseModel, Field

# LangChainã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒŠãƒ«ï¼‰
try:
    from langchain_openai import ChatOpenAI
    LANGCHAIN_AVAILABLE = True
except ImportError:
    LANGCHAIN_AVAILABLE = False


# ========================================
# Pydantic ãƒ¢ãƒ‡ãƒ«ï¼ˆLLM Structured Outputç”¨ï¼‰
# ========================================

class FailureAnalysis(BaseModel):
    """LLMãŒå‡ºåŠ›ã™ã‚‹å¤±æ•—åˆ†æï¼ˆå½¢å¼å›ºå®šï¼‰"""
    
    failure_category: Literal[
        "APPIUM_CONNECTION_ERROR",    # Appiumæ¥ç¶šã‚¨ãƒ©ãƒ¼
        "ELEMENT_NOT_FOUND",          # è¦ç´ ãŒè¦‹ã¤ã‹ã‚‰ãªã„
        "VERIFICATION_FAILED",        # æ¤œè¨¼å¤±æ•—ï¼ˆLLMåˆ¤å®šï¼‰
        "TIMEOUT",                    # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
        "LLM_JUDGMENT_ERROR",         # LLMåˆ¤å®šãƒŸã‚¹
        "APP_CRASH",                  # ã‚¢ãƒ—ãƒªã‚¯ãƒ©ãƒƒã‚·ãƒ¥
        "SESSION_ERROR",              # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¨ãƒ©ãƒ¼
        "UNKNOWN"                     # ä¸æ˜
    ] = Field(description="å¤±æ•—ã‚«ãƒ†ã‚´ãƒª")
    
    summary: str = Field(description="å¤±æ•—æ¦‚è¦ï¼ˆ1æ–‡ã§ç°¡æ½”ã«ï¼‰")
    
    root_causes: List[str] = Field(
        description="æŠ€è¡“çš„ãªåŸå› ï¼ˆ1-3å€‹ï¼‰",
        min_length=1,
        max_length=3
    )
    
    recommendations: List[str] = Field(
        description="å…·ä½“çš„ãªå¯¾å‡¦æ³•ï¼ˆå„ªå…ˆåº¦é †ã€1-3å€‹ï¼‰",
        min_length=1,
        max_length=3
    )
    
    confidence: Literal["HIGH", "MEDIUM", "LOW"] = Field(
        description="åˆ†æã®ç¢ºä¿¡åº¦"
    )
    
    def to_plaintext(self) -> str:
        """åˆ†æçµæœã‚’ãƒ—ãƒ¬ãƒ¼ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ã§å‡ºåŠ›"""
        category_display = CATEGORY_DISPLAY.get(self.failure_category, self.failure_category)
        
        root_causes_text = "\n".join(f"  - {cause}" for cause in self.root_causes)
        recommendations_text = "\n".join(f"  {i}. {rec}" for i, rec in enumerate(self.recommendations, 1))
        
        return f"""---
ãƒ†ã‚¹ãƒˆå¤±æ•—ã®åŸå› åˆ†æ:

ã‚«ãƒ†ã‚´ãƒª: {category_display}

æ¦‚è¦:
{self.summary}

æ¨å®šåŸå› :
{root_causes_text}

æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³:
{recommendations_text}

åˆ†æç¢ºä¿¡åº¦: {self.confidence}
---"""


# ã‚«ãƒ†ã‚´ãƒªè¡¨ç¤ºå
CATEGORY_DISPLAY = {
    "APPIUM_CONNECTION_ERROR": "ğŸ”Œ Appiumæ¥ç¶šã‚¨ãƒ©ãƒ¼",
    "ELEMENT_NOT_FOUND": "ğŸ” è¦ç´ ãŒè¦‹ã¤ã‹ã‚‰ãªã„",
    "VERIFICATION_FAILED": "âŒ æ¤œè¨¼å¤±æ•—",
    "TIMEOUT": "â±ï¸ ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ",
    "LLM_JUDGMENT_ERROR": "ğŸ¤– LLMåˆ¤å®šãƒŸã‚¹",
    "APP_CRASH": "ğŸ’¥ ã‚¢ãƒ—ãƒªã‚¯ãƒ©ãƒƒã‚·ãƒ¥",
    "SESSION_ERROR": "ğŸ”— ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¨ãƒ©ãƒ¼",
    "UNKNOWN": "â“ ä¸æ˜",
}


# ========================================
# ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹
# ========================================

@dataclass
class FailedTestInfo:
    """å¤±æ•—ã—ãŸãƒ†ã‚¹ãƒˆã®æƒ…å ±"""
    test_id: str
    title: str
    steps: str
    expected: str
    
    # å¤±æ•—æƒ…å ±
    failed_step: Optional[str] = None
    error_message: Optional[str] = None
    error_type: Optional[str] = None
    failure_timestamp: Optional[str] = None
    
    # LLMæ¤œè¨¼æƒ…å ±
    verification_phase1: Optional[Dict[str, Any]] = None
    verification_phase2: Optional[Dict[str, Any]] = None
    
    # ç”»é¢æƒ…å ±
    last_screen_type: Optional[str] = None
    last_screen_xml: Optional[str] = None
    screenshots: List[Dict[str, str]] = field(default_factory=list)
    
    # é€²æ—æƒ…å ±
    progress_summary: Optional[str] = None
    completed_steps: List[str] = field(default_factory=list)
    
    # ãƒ­ã‚°è¡Œç¯„å›²
    log_start_line: int = 0
    log_end_line: int = 0
    
    # LLMåˆ†æçµæœ
    analysis: Optional[FailureAnalysis] = None


# ========================================
# ãƒ¡ã‚¤ãƒ³ã‚¯ãƒ©ã‚¹
# ========================================

class FailureReportGenerator:
    """ãƒ†ã‚¹ãƒˆå¤±æ•—ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå™¨ï¼ˆLLMåˆ†æã‚’ä½¿ç”¨ï¼‰"""
    
    def __init__(
        self,
        log_dir: Path,
        model_name: str = "gpt-4.1-mini"
    ):
        """
        Args:
            log_dir: ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆrun_YYYYMMDD_HHMMSSï¼‰
            model_name: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«å
        """
        self.log_dir = Path(log_dir)
        self.model_name = model_name
        
        # JSONLãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™
        jsonl_files = list(self.log_dir.glob("*.jsonl"))
        if not jsonl_files:
            raise FileNotFoundError(f"JSONLãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.log_dir}")
        self.log_file = jsonl_files[0]
        
        # ãƒ­ã‚°ã‚’ãƒ­ãƒ¼ãƒ‰
        self.entries: List[Dict[str, Any]] = []
        self._load_log()
        
        # å…¨ãƒ†ã‚¹ãƒˆæƒ…å ±ã‚’æŠ½å‡º
        self.all_tests: List[Dict[str, Any]] = []
        self._extract_all_tests()
        
        # å¤±æ•—ãƒ†ã‚¹ãƒˆã‚’æŠ½å‡º
        self.failed_tests: List[FailedTestInfo] = []
        self._extract_failed_tests()
    
    def _load_log(self):
        """ãƒ­ã‚°ã‚’èª­ã¿è¾¼ã‚€"""
        with open(self.log_file, "r", encoding="utf-8") as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if line:
                    try:
                        entry = json.loads(line)
                        entry["_line_num"] = line_num
                        self.entries.append(entry)
                    except json.JSONDecodeError:
                        pass
    
    def _extract_all_tests(self):
        """å…¨ãƒ†ã‚¹ãƒˆæƒ…å ±ã‚’æŠ½å‡º
        
        çµæœã®å„ªå…ˆé †ä½: skip > fail > pass > unknown
        - SKIP ã¯æœ€çµ‚åˆ¤å®šã¨ã—ã¦ FAIL ã‚’ä¸Šæ›¸ãã§ãã‚‹
        - FAIL ã¯ PASS ã‚’ä¸Šæ›¸ãã§ãã‚‹ï¼ˆä¾‹: PASSå¾Œã«ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿï¼‰
        """
        current_test_id = None
        
        # çµæœã®å„ªå…ˆé †ä½ï¼ˆæ•°å€¤ãŒå¤§ãã„ã»ã©å„ªå…ˆï¼‰
        RESULT_PRIORITY = {"unknown": 0, "pass": 1, "fail": 2, "skip": 3}
        
        def update_test_result(test_id: str, new_result: str):
            """ãƒ†ã‚¹ãƒˆçµæœã‚’æ›´æ–°ï¼ˆå„ªå…ˆé †ä½ãŒé«˜ã„å ´åˆã®ã¿ï¼‰"""
            for test in reversed(self.all_tests):
                if test["test_id"] == test_id:
                    current_priority = RESULT_PRIORITY.get(test["result"], 0)
                    new_priority = RESULT_PRIORITY.get(new_result, 0)
                    if new_priority > current_priority:
                        test["result"] = new_result
                    break
        
        for entry in self.entries:
            cat = entry.get("cat", "")
            evt = entry.get("evt", "")
            data = entry.get("data", {}) or {}
            
            # ãƒ†ã‚¹ãƒˆé–‹å§‹
            if cat == "TEST" and evt == "START" and "test_id" in data:
                test_id = data.get("test_id", "")
                # session ã¯é™¤å¤–ã—ã€TEST_XXXX å½¢å¼ã®ã¿å¯¾è±¡
                if test_id and test_id.startswith("TEST_"):
                    current_test_id = test_id
                    self.all_tests.append({
                        "test_id": test_id,
                        "title": data.get("title", ""),
                        "result": "unknown"  # å¾Œã§æ›´æ–°
                    })
            
            # ãƒ†ã‚¹ãƒˆçµæœ - COMPLETE ã‚¤ãƒ™ãƒ³ãƒˆã§ status ã‚’ç¢ºèª
            if cat == "TEST" and evt == "COMPLETE" and current_test_id:
                status = data.get("status", "")
                if status == "RESULT_PASS":
                    update_test_result(current_test_id, "pass")
                elif status == "RESULT_SKIP":
                    update_test_result(current_test_id, "skip")
            
            # ãƒ†ã‚¹ãƒˆå¤±æ•— - FAIL ã‚¤ãƒ™ãƒ³ãƒˆ
            if cat == "TEST" and evt == "FAIL" and current_test_id:
                update_test_result(current_test_id, "fail")
            
            # ã‚¹ã‚­ãƒƒãƒ— - SKIP ã‚¤ãƒ™ãƒ³ãƒˆï¼ˆå¾“æ¥ã®æ–¹å¼ã‚‚ã‚µãƒãƒ¼ãƒˆï¼‰
            if cat == "TEST" and evt == "SKIP" and current_test_id:
                update_test_result(current_test_id, "skip")

    def _extract_failed_tests(self):
        """å¤±æ•—ã—ãŸãƒ†ã‚¹ãƒˆã‚’æŠ½å‡º"""
        current_test: Optional[FailedTestInfo] = None
        current_test_start_line = 0
        
        for entry in self.entries:
            cat = entry.get("cat", "")
            evt = entry.get("evt", "")
            data = entry.get("data", {}) or {}
            line_num = entry.get("_line_num", 0)
            
            # ãƒ†ã‚¹ãƒˆé–‹å§‹
            if cat == "TEST" and evt == "START" and "test_id" in data:
                if current_test and current_test.error_message:
                    # å‰ã®ãƒ†ã‚¹ãƒˆãŒå¤±æ•—ã—ã¦ã„ãŸã‚‰ä¿å­˜
                    current_test.log_end_line = line_num - 1
                    self.failed_tests.append(current_test)
                
                current_test = FailedTestInfo(
                    test_id=data.get("test_id", ""),
                    title=data.get("title", ""),
                    steps=data.get("steps", ""),
                    expected=data.get("expected", ""),
                    log_start_line=line_num
                )
                current_test_start_line = line_num
            
            if current_test is None:
                continue
            
            # ç”»é¢ã‚¿ã‚¤ãƒ—
            if cat == "SCREEN" and evt == "COMPLETE":
                current_test.last_screen_type = data.get("screen_type")
            
            # ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆ
            if cat == "SCREEN" and evt == "UPDATE" and "image_path" in data:
                current_test.screenshots.append({
                    "path": data.get("image_path", ""),
                    "filename": data.get("image_filename", ""),
                    "label": data.get("label"),
                    "timestamp": entry.get("ts", "")
                })
            
            # XMLï¼ˆLLMãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‹ã‚‰æŠ½å‡ºï¼‰
            if cat == "LLM" and evt == "START":
                user_prompt = data.get("user_prompt", "")
                if "<hierarchy" in user_prompt:
                    # XMLã‚’æŠ½å‡º
                    match = re.search(r'(<hierarchy.*?</hierarchy>)', user_prompt, re.DOTALL)
                    if match:
                        current_test.last_screen_xml = match.group(1)
            
            # æ¤œè¨¼çµæœ
            if cat == "LLM" and evt == "VERIFY_RESPONSE":
                phase = data.get("phase")
                if phase == 1:
                    current_test.verification_phase1 = data
                elif phase == 2:
                    current_test.verification_phase2 = data
            
            # é€²æ—ã‚µãƒãƒªãƒ¼
            if cat == "OBJECTIVE" and evt == "UPDATE":
                summary = data.get("summary")
                if summary:
                    current_test.progress_summary = summary
            
            # ã‚¹ãƒ†ãƒƒãƒ—å®Œäº†
            if cat == "STEP" and evt == "COMPLETE" and data.get("success"):
                step = data.get("step", "")
                if step:
                    current_test.completed_steps.append(step)
            
            # ã‚¹ãƒ†ãƒƒãƒ—å¤±æ•—
            if cat == "STEP" and evt == "FAIL":
                current_test.failed_step = data.get("step", "")
                # error ã¾ãŸã¯ reason ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‹ã‚‰ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å–å¾—
                error = data.get("error", "") or data.get("reason", "")
                if error:
                    current_test.error_message = error
                
                    # ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—ã‚’æŠ½å‡º
                    if "cannot be proxied" in error or "instrumentation process" in error:
                        current_test.error_type = "AppiumConnectionError"
                    elif "InvalidContextError" in error:
                        current_test.error_type = "InvalidContextError"
                    elif "TimeoutError" in error or "timeout" in error.lower():
                        current_test.error_type = "TimeoutError"
                    elif "NoSuchElement" in error or "not found" in error.lower() or "è¦ç´ ãŒè¦‹ã¤ã‹ã‚‰" in error or "å­˜åœ¨ã—ãª" in error:
                        current_test.error_type = "NoSuchElementError"
                    else:
                        current_test.error_type = "UnknownError"
            
            # ãƒ†ã‚¹ãƒˆå¤±æ•—
            if cat == "TEST" and evt == "FAIL":
                current_test.failure_timestamp = entry.get("ts", "")
                if not current_test.error_message:
                    # data.error ã¾ãŸã¯ data.analysisï¼ˆåˆ†æçµæœï¼‰ã‚’ç¢ºèª
                    error = data.get("error", "") or data.get("analysis", "")
                    if error and error != "ãƒ†ã‚¹ãƒˆå¤±æ•— - åŸå› åˆ†æçµæœ":
                        current_test.error_message = error
            
            # ãƒ†ã‚¹ãƒˆçµ‚äº†ï¼ˆæ¬¡ã®ãƒ†ã‚¹ãƒˆãŒå§‹ã¾ã‚‹ã‹ã€ã‚»ãƒƒã‚·ãƒ§ãƒ³çµ‚äº†ï¼‰
            if cat == "SESSION" and evt == "END":
                if current_test and current_test.error_message:
                    current_test.log_end_line = line_num
                    self.failed_tests.append(current_test)
                    current_test = None
        
        # æœ€å¾Œã®ãƒ†ã‚¹ãƒˆ
        if current_test and current_test.error_message:
            current_test.log_end_line = len(self.entries)
            self.failed_tests.append(current_test)
    
    def _analyze_with_llm(self, test_info: FailedTestInfo) -> Optional[FailureAnalysis]:
        """LLMã‚’ä½¿ç”¨ã—ã¦å¤±æ•—ã‚’åˆ†æ"""
        try:
            from langchain_openai import ChatOpenAI
            
            llm = ChatOpenAI(
                model=self.model_name,
                temperature=0,
                timeout=30,
                max_retries=2
            )
            
            # Structured Outputã‚’ä½¿ç”¨
            structured_llm = llm.with_structured_output(FailureAnalysis)
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
            prompt = self._build_analysis_prompt(test_info)
            
            # LLMå‘¼ã³å‡ºã—
            result = structured_llm.invoke(prompt)
            return result
            
        except Exception as e:
            print(f"âš ï¸ LLMåˆ†æã‚¨ãƒ©ãƒ¼ ({test_info.test_id}): {e}")
            return None
    
    def _build_analysis_prompt(self, test_info: FailedTestInfo) -> str:
        """åˆ†æç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰"""
        prompt = f"""ã‚ãªãŸã¯ãƒ¢ãƒã‚¤ãƒ«ã‚¢ãƒ—ãƒªãƒ†ã‚¹ãƒˆè‡ªå‹•åŒ–ã®å°‚é–€å®¶ã§ã™ã€‚
ä»¥ä¸‹ã®ãƒ†ã‚¹ãƒˆå¤±æ•—ã‚’åˆ†æã—ã€æ§‹é€ åŒ–ã•ã‚ŒãŸåˆ†æçµæœã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚

## ãƒ†ã‚¹ãƒˆæƒ…å ±
- **ãƒ†ã‚¹ãƒˆID**: {test_info.test_id}
- **ãƒ†ã‚¹ãƒˆå**: {test_info.title}
- **ãƒ†ã‚¹ãƒˆæ‰‹é †**:
{test_info.steps}
- **æœŸå¾…çµæœ**: {test_info.expected}

## é€²æ—çŠ¶æ³
- å®Œäº†ã‚¹ãƒ†ãƒƒãƒ—: {len(test_info.completed_steps)}
- å¤±æ•—ã—ãŸã‚¹ãƒ†ãƒƒãƒ—: {test_info.failed_step or "ä¸æ˜"}
"""
        
        if test_info.progress_summary:
            prompt += f"\n### é€²æ—ã‚µãƒãƒªãƒ¼\n{test_info.progress_summary}\n"
        
        if test_info.last_screen_type:
            prompt += f"\n## ç›´å‰ã®ç”»é¢çŠ¶æ…‹\n- ç”»é¢ã‚¿ã‚¤ãƒ—: {test_info.last_screen_type}\n"
        
        prompt += f"""
## ã‚¨ãƒ©ãƒ¼æƒ…å ±
- **ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—**: {test_info.error_type or "ä¸æ˜"}
- **ã‚¨ãƒ©ãƒ¼å†…å®¹**: {test_info.error_message[:500] if test_info.error_message else "ä¸æ˜"}
"""
        
        if test_info.verification_phase1:
            prompt += f"""
## LLMæ¤œè¨¼çµæœï¼ˆPhase 1ï¼‰
- success: {test_info.verification_phase1.get("success")}
- reason: {test_info.verification_phase1.get("reason", "")[:300]}
"""
        
        if test_info.verification_phase2:
            prompt += f"""
## LLMæ¤œè¨¼çµæœï¼ˆPhase 2ï¼‰
- verified: {test_info.verification_phase2.get("verified")}
- confidence: {test_info.verification_phase2.get("confidence")}
- reason: {test_info.verification_phase2.get("reason", "")[:300]}
- discrepancy: {test_info.verification_phase2.get("discrepancy")}
"""
        
        prompt += """
## åˆ†æã®è¦³ç‚¹
1. **failure_category**: æœ€ã‚‚è©²å½“ã™ã‚‹ã‚«ãƒ†ã‚´ãƒªã‚’1ã¤é¸æŠ
   - APPIUM_CONNECTION_ERROR: Appiumã‚µãƒ¼ãƒãƒ¼ã¨ã®æ¥ç¶šå•é¡Œ
   - ELEMENT_NOT_FOUND: ç”»é¢è¦ç´ ãŒè¦‹ã¤ã‹ã‚‰ãªã„
   - VERIFICATION_FAILED: LLMã«ã‚ˆã‚‹ç”»é¢æ¤œè¨¼ãŒå¤±æ•—
   - TIMEOUT: æ“ä½œã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
   - LLM_JUDGMENT_ERROR: LLMã®åˆ¤æ–­ãƒŸã‚¹
   - APP_CRASH: ã‚¢ãƒ—ãƒªã®ã‚¯ãƒ©ãƒƒã‚·ãƒ¥
   - SESSION_ERROR: Appiumã‚»ãƒƒã‚·ãƒ§ãƒ³ã®å•é¡Œ
   - UNKNOWN: ä¸Šè¨˜ã«è©²å½“ã—ãªã„

2. **summary**: ä½•ãŒèµ·ããŸã‹ã‚’1æ–‡ã§ï¼ˆæŠ€è¡“ç”¨èªã‚’ä½¿ã‚ãšç°¡æ½”ã«ï¼‰

3. **root_causes**: æŠ€è¡“çš„ãªåŸå› ï¼ˆ1-3å€‹ã€ç®‡æ¡æ›¸ãç”¨ï¼‰

4. **recommendations**: å…·ä½“çš„ãªå¯¾å‡¦æ³•ï¼ˆå„ªå…ˆåº¦é †ã€1-3å€‹ã€å®Ÿè¡Œå¯èƒ½ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ï¼‰

5. **confidence**: åˆ†æã®ç¢ºä¿¡åº¦
   - HIGH: ãƒ­ã‚°ã‹ã‚‰åŸå› ãŒæ˜ç¢ºã«ç‰¹å®šã§ãã‚‹
   - MEDIUM: åŸå› ã¯æ¨å®šã§ãã‚‹ãŒç¢ºå®šã§ã¯ãªã„
   - LOW: æƒ…å ±ãŒä¸è¶³ã—ã¦ãŠã‚Šæ¨æ¸¬ã®è¦ç´ ãŒå¤§ãã„

## é‡è¦
- æ¨æ¸¬ã¯é¿ã‘ã€ãƒ­ã‚°ã‹ã‚‰èª­ã¿å–ã‚Œã‚‹äº‹å®Ÿã«åŸºã¥ãã“ã¨
- ãƒªã‚³ãƒ¡ãƒ³ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã¯å®Ÿè¡Œå¯èƒ½ãªå…·ä½“çš„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã«ã™ã‚‹ã“ã¨
"""
        return prompt
    
    def _fallback_analysis(self, test_info: FailedTestInfo) -> FailureAnalysis:
        """LLMã‚’ä½¿ç”¨ã—ãªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯åˆ†æ"""
        error = test_info.error_message or ""
        error_lower = error.lower()
        
        # ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã«åŸºã¥ãåˆ†é¡
        if "cannot be proxied" in error or "instrumentation process" in error:
            return FailureAnalysis(
                failure_category="APPIUM_CONNECTION_ERROR",
                summary="Appiumã‚µãƒ¼ãƒãƒ¼ã¨ã®é€šä¿¡ãŒæ–­çµ¶ã—ã¾ã—ãŸ",
                root_causes=[
                    "UiAutomator2ã®instrumentationãƒ—ãƒ­ã‚»ã‚¹ãŒã‚¯ãƒ©ãƒƒã‚·ãƒ¥",
                    "Androidç«¯æœ«ã¨ã®æ¥ç¶šãŒä¸å®‰å®š"
                ],
                recommendations=[
                    "Androidç«¯æœ«/ã‚¨ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ã‚’å†èµ·å‹•ã™ã‚‹",
                    "Appiumã‚µãƒ¼ãƒãƒ¼ã‚’å†èµ·å‹•ã™ã‚‹",
                    "adb devicesã§æ¥ç¶šçŠ¶æ…‹ã‚’ç¢ºèªã™ã‚‹"
                ],
                confidence="HIGH"
            )
        elif any(p in error for p in ["NoSuchElement", "è¦ç´ ãŒè¦‹ã¤ã‹ã‚‰", "å­˜åœ¨ã—ãª", "è¦‹ã¤ã‹ã‚‰ãª", "å­˜åœ¨ã›ãš", "ä¸åœ¨"]) or "not found" in error_lower:
            return FailureAnalysis(
                failure_category="ELEMENT_NOT_FOUND",
                summary="ç”»é¢ä¸Šã§æŒ‡å®šã—ãŸè¦ç´ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ",
                root_causes=[
                    "è¦ç´ ã®ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ãŒæ­£ã—ããªã„",
                    "ç”»é¢é·ç§»ãŒå®Œäº†ã—ã¦ã„ãªã„",
                    "è¦ç´ ãŒç”»é¢å¤–ã«ã‚ã‚‹"
                ],
                recommendations=[
                    "è¦ç´ ã®XPathã‚„resource-idã‚’ç¢ºèªã™ã‚‹",
                    "å¾…æ©Ÿæ™‚é–“ã‚’å¢—ã‚„ã™",
                    "ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã§è¦ç´ ã‚’è¡¨ç¤ºã™ã‚‹"
                ],
                confidence="MEDIUM"
            )
        elif "timeout" in error_lower or "ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ" in error:
            return FailureAnalysis(
                failure_category="TIMEOUT",
                summary="æ“ä½œãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸ",
                root_causes=["å‡¦ç†ã«æ™‚é–“ãŒã‹ã‹ã‚Šã™ããŸ"],
                recommendations=["ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå€¤ã‚’å¢—ã‚„ã™"],
                confidence="MEDIUM"
            )
        elif any(p in error for p in ["æ¤œè¨¼å¤±æ•—", "ç¢ºèªã§ããª", "æœŸå¾…", "åˆ¤å®šåŸºæº–"]):
            return FailureAnalysis(
                failure_category="VERIFICATION_FAILED",
                summary="ãƒ†ã‚¹ãƒˆçµæœã®æ¤œè¨¼ãŒå¤±æ•—ã—ã¾ã—ãŸ",
                root_causes=[
                    "æœŸå¾…ã•ã‚Œã‚‹çŠ¶æ…‹ã¨å®Ÿéš›ã®çŠ¶æ…‹ãŒç•°ãªã‚‹",
                    "æ¤œè¨¼æ¡ä»¶ãŒå³ã—ã™ãã‚‹"
                ],
                recommendations=[
                    "æœŸå¾…çµæœã‚’å†ç¢ºèªã™ã‚‹",
                    "æ¤œè¨¼ãƒ­ã‚¸ãƒƒã‚¯ã‚’è¦‹ç›´ã™"
                ],
                confidence="MEDIUM"
            )
        elif any(p in error for p in ["ã‚¯ãƒ©ãƒƒã‚·ãƒ¥", "crash", "ANR"]) or "crash" in error_lower:
            return FailureAnalysis(
                failure_category="APP_CRASH",
                summary="ã‚¢ãƒ—ãƒªãŒã‚¯ãƒ©ãƒƒã‚·ãƒ¥ã—ã¾ã—ãŸ",
                root_causes=["ã‚¢ãƒ—ãƒªå†…éƒ¨ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ"],
                recommendations=[
                    "logcatã§ã‚¯ãƒ©ãƒƒã‚·ãƒ¥ãƒ­ã‚°ã‚’ç¢ºèªã™ã‚‹",
                    "ã‚¢ãƒ—ãƒªé–‹ç™ºè€…ã«å ±å‘Šã™ã‚‹"
                ],
                confidence="HIGH"
            )
        else:
            return FailureAnalysis(
                failure_category="UNKNOWN",
                summary="ãƒ†ã‚¹ãƒˆãŒå¤±æ•—ã—ã¾ã—ãŸï¼ˆè©³ç´°ã¯ãƒ­ã‚°ã‚’ç¢ºèªï¼‰",
                root_causes=["è©³ç´°ãªãƒ­ã‚°ç¢ºèªãŒå¿…è¦"],
                recommendations=["è©³ç´°ãƒ­ã‚°ã‚’ç¢ºèªã—ã¦åŸå› ã‚’ç‰¹å®šã™ã‚‹"],
                confidence="LOW"
            )
    
    def _analyze_failure_trends(self) -> Optional[str]:
        """LLMã‚’ä½¿ç”¨ã—ã¦å¤±æ•—å‚¾å‘ã‚’åˆ†æ"""
        if not self.failed_tests:
            return None
        
        try:
            from langchain_openai import ChatOpenAI
            
            llm = ChatOpenAI(
                model=self.model_name,
                temperature=0,
                timeout=60,
                max_retries=2
            )
            
            # å¤±æ•—ãƒ†ã‚¹ãƒˆã®ã‚µãƒãƒªãƒ¼ã‚’æ§‹ç¯‰
            failure_summaries = []
            category_counts: Dict[str, int] = {}
            
            for test in self.failed_tests:
                if test.analysis:
                    cat = test.analysis.failure_category
                    category_counts[cat] = category_counts.get(cat, 0) + 1
                    failure_summaries.append(f"- {test.test_id}: {test.analysis.summary}")
            
            # ã‚«ãƒ†ã‚´ãƒªåˆ¥é›†è¨ˆãƒ†ã‚­ã‚¹ãƒˆ
            category_text = "\n".join(
                f"- {CATEGORY_DISPLAY.get(cat, cat)}: {count}ä»¶"
                for cat, count in sorted(category_counts.items(), key=lambda x: -x[1])
            )
            
            prompt = f"""ã‚ãªãŸã¯ãƒ¢ãƒã‚¤ãƒ«ã‚¢ãƒ—ãƒªãƒ†ã‚¹ãƒˆè‡ªå‹•åŒ–ã®å°‚é–€å®¶ã§ã™ã€‚
ä»¥ä¸‹ã®ãƒ†ã‚¹ãƒˆå¤±æ•—ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã—ã€å‚¾å‘ã¨ç‰¹å¾´ã‚’æ—¥æœ¬èªã§ç°¡æ½”ã«ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚

## ãƒ†ã‚¹ãƒˆå®Ÿè¡Œçµæœ
- ãƒ†ã‚¹ãƒˆç·æ•°: {len(self.all_tests)}
- æˆåŠŸ: {sum(1 for t in self.all_tests if t['result'] == 'pass')}
- å¤±æ•—: {len(self.failed_tests)}

## å¤±æ•—ã‚«ãƒ†ã‚´ãƒªåˆ¥é›†è¨ˆ
{category_text}

## å„å¤±æ•—ãƒ†ã‚¹ãƒˆã®æ¦‚è¦
{chr(10).join(failure_summaries[:20])}  # æœ€å¤§20ä»¶

## å‡ºåŠ›å½¢å¼
ä»¥ä¸‹ã®è¦³ç‚¹ã§åˆ†æã—ã¦ãã ã•ã„ï¼ˆMarkdownå½¢å¼ã€å„é …ç›®2-3æ–‡ç¨‹åº¦ï¼‰:

### ä¸»ãªå¤±æ•—ãƒ‘ã‚¿ãƒ¼ãƒ³
ã©ã®ã‚ˆã†ãªç¨®é¡ã®å¤±æ•—ãŒå¤šã„ã‹ã€ãã®å‚¾å‘ã‚’èª¬æ˜

### ç‰¹å¾´çš„ãªå¤±æ•—
é€šå¸¸ã¨ç•°ãªã‚‹å¤±æ•—ã‚„ã€æ³¨ç›®ã™ã¹ãå¤±æ•—ãŒã‚ã‚Œã°è¨˜è¼‰

### æ”¹å–„ææ¡ˆ
ãƒ†ã‚¹ãƒˆæˆåŠŸç‡ã‚’ä¸Šã’ã‚‹ãŸã‚ã®å…·ä½“çš„ãªææ¡ˆï¼ˆå„ªå…ˆåº¦é †ã«2-3å€‹ï¼‰
"""
            
            response = llm.invoke(prompt)
            return response.content
            
        except Exception as e:
            print(f"âš ï¸ å‚¾å‘åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return None

    def generate_report(self) -> Path:
        """Markdownãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        if not self.failed_tests:
            print("âœ… å¤±æ•—ã—ãŸãƒ†ã‚¹ãƒˆã¯ã‚ã‚Šã¾ã›ã‚“")
            return self._generate_empty_report()
        
        # LLMåˆ†æã‚’å®Ÿè¡Œ
        print(f"ğŸ¤– LLMåˆ†æã‚’å®Ÿè¡Œä¸­... ({len(self.failed_tests)}ä»¶)")
        for i, test_info in enumerate(self.failed_tests, 1):
            print(f"  [{i}/{len(self.failed_tests)}] {test_info.test_id}...")
            test_info.analysis = self._analyze_with_llm(test_info)
            if test_info.analysis is None:
                test_info.analysis = self._fallback_analysis(test_info)
        
        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        report_content = self._build_report()
        
        # ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›
        report_path = self.log_dir / "failure_report.md"
        with open(report_path, "w", encoding="utf-8") as f:
            f.write(report_content)
        
        print(f"âœ… ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¾ã—ãŸ: {report_path}")
        return report_path
    
    def _generate_empty_report(self) -> Path:
        """å¤±æ•—ãƒ†ã‚¹ãƒˆãŒãªã„å ´åˆã®ãƒ¬ãƒãƒ¼ãƒˆ"""
        content = f"""# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œãƒ¬ãƒãƒ¼ãƒˆ

**å®Ÿè¡Œæ—¥æ™‚**: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}  
**ãƒ­ã‚°ãƒ•ã‚©ãƒ«ãƒ€**: `{self.log_dir.name}`

---

## çµæœã‚µãƒãƒªãƒ¼

âœ… **ã™ã¹ã¦ã®ãƒ†ã‚¹ãƒˆãŒæˆåŠŸã—ã¾ã—ãŸ**

å¤±æ•—ã—ãŸãƒ†ã‚¹ãƒˆã¯ã‚ã‚Šã¾ã›ã‚“ã€‚
"""
        report_path = self.log_dir / "failure_report.md"
        with open(report_path, "w", encoding="utf-8") as f:
            f.write(content)
        return report_path
    
    def _build_report(self) -> str:
        """ãƒ¬ãƒãƒ¼ãƒˆæœ¬ä½“ã‚’æ§‹ç¯‰"""
        lines = []
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼
        run_time = self.log_dir.name.replace("run_", "").replace("_", " ")
        lines.append("# ãƒ†ã‚¹ãƒˆå¤±æ•—ãƒ¬ãƒãƒ¼ãƒˆ")
        lines.append("")
        lines.append(f"**å®Ÿè¡Œæ—¥æ™‚**: {run_time}  ")
        lines.append(f"**ãƒ­ã‚°ãƒ•ã‚©ãƒ«ãƒ€**: `{self.log_dir.name}`")
        lines.append("")
        lines.append("---")
        lines.append("")
        
        # ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼
        total_tests = len(self.all_tests)
        passed_tests = sum(1 for t in self.all_tests if t["result"] == "pass")
        failed_tests = sum(1 for t in self.all_tests if t["result"] == "fail")
        skipped_tests = sum(1 for t in self.all_tests if t["result"] == "skip")
        unknown_tests = total_tests - passed_tests - failed_tests - skipped_tests
        
        lines.append("## ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼")
        lines.append("")
        lines.append("| é …ç›® | å€¤ |")
        lines.append("|------|-----|")
        lines.append(f"| ãƒ†ã‚¹ãƒˆç·æ•° | {total_tests} |")
        lines.append(f"| âœ… æˆåŠŸ | {passed_tests} |")
        lines.append(f"| âŒ å¤±æ•— | {failed_tests} |")
        if skipped_tests > 0:
            lines.append(f"| â­ï¸ ã‚¹ã‚­ãƒƒãƒ— | {skipped_tests} |")
        if unknown_tests > 0:
            lines.append(f"| â“ ä¸æ˜ | {unknown_tests} |")
        
        # æˆåŠŸç‡ã‚’è¨ˆç®—
        if total_tests > 0:
            success_rate = (passed_tests / total_tests) * 100
            lines.append(f"| **æˆåŠŸç‡** | **{success_rate:.1f}%** |")
        
        lines.append("")
        lines.append("---")
        lines.append("")
        
        # å¤±æ•—ã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚µãƒãƒªãƒ¼
        lines.append("## å¤±æ•—ã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚µãƒãƒªãƒ¼")
        lines.append("")
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥é›†è¨ˆ
        category_counts: Dict[str, int] = {}
        for test in self.failed_tests:
            if test.analysis:
                cat = test.analysis.failure_category
                category_counts[cat] = category_counts.get(cat, 0) + 1
        
        if category_counts:
            lines.append("| ã‚«ãƒ†ã‚´ãƒª | ä»¶æ•° |")
            lines.append("|------|-----|")
            for cat, count in sorted(category_counts.items(), key=lambda x: -x[1]):
                display = CATEGORY_DISPLAY.get(cat, cat)
                lines.append(f"| {display} | {count} |")
            lines.append("")
        else:
            lines.append("å¤±æ•—ãƒ†ã‚¹ãƒˆã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
            lines.append("")
        
        lines.append("---")
        lines.append("")
        
        # LLMã«ã‚ˆã‚‹å‚¾å‘åˆ†æ
        if self.failed_tests:
            trend_analysis = self._analyze_failure_trends()
            if trend_analysis:
                lines.append("## å¤±æ•—å‚¾å‘åˆ†æ")
                lines.append("")
                lines.append(trend_analysis)
                lines.append("")
                lines.append("---")
                lines.append("")
        
        # å„å¤±æ•—ãƒ†ã‚¹ãƒˆã®è©³ç´°
        lines.append("## å¤±æ•—ãƒ†ã‚¹ãƒˆä¸€è¦§")
        lines.append("")
        
        for test_info in self.failed_tests:
            lines.extend(self._build_test_section(test_info))
            lines.append("")
        
        return "\n".join(lines)
    
    def _build_test_section(self, test_info: FailedTestInfo) -> List[str]:
        """å„ãƒ†ã‚¹ãƒˆã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ§‹ç¯‰"""
        lines = []
        analysis = test_info.analysis
        
        # ãƒ†ã‚¹ãƒˆãƒ˜ãƒƒãƒ€ãƒ¼
        lines.append(f"### {test_info.test_id}: {test_info.title}")
        lines.append("")
        
        # åŸºæœ¬æƒ…å ±ãƒ†ãƒ¼ãƒ–ãƒ«
        lines.append("| é …ç›® | å†…å®¹ |")
        lines.append("|------|------|")
        
        if analysis:
            category_display = CATEGORY_DISPLAY.get(analysis.failure_category, analysis.failure_category)
            lines.append(f"| **çµæœ** | {category_display} |")
        
        if test_info.failure_timestamp:
            time_only = test_info.failure_timestamp[11:19] if len(test_info.failure_timestamp) >= 19 else test_info.failure_timestamp
            lines.append(f"| **å¤±æ•—æ™‚åˆ»** | {time_only} |")
        
        if test_info.failed_step:
            step_display = test_info.failed_step[:50] + "..." if len(test_info.failed_step) > 50 else test_info.failed_step
            lines.append(f"| **å¤±æ•—ã‚¹ãƒ†ãƒƒãƒ—** | {step_display} |")
        
        if analysis:
            lines.append(f"| **ä¿¡é ¼åº¦** | {analysis.confidence} |")
        
        lines.append("")
        
        # å¤±æ•—æ¦‚è¦
        if analysis:
            lines.append("#### å¤±æ•—æ¦‚è¦")
            lines.append("")
            lines.append(analysis.summary)
            lines.append("")
        
        # åŸå› è©³ç´°
        if analysis and analysis.root_causes:
            lines.append("#### åŸå› è©³ç´°")
            lines.append("")
            for cause in analysis.root_causes:
                lines.append(f"- {cause}")
            lines.append("")
        
        # æ¨å¥¨å¯¾å¿œ
        if analysis and analysis.recommendations:
            lines.append("#### æ¨å¥¨å¯¾å¿œ")
            lines.append("")
            for i, rec in enumerate(analysis.recommendations, 1):
                lines.append(f"{i}. {rec}")
            lines.append("")
        
        # å‚ç…§ãƒ­ã‚°
        lines.append("#### å‚ç…§ãƒ­ã‚°")
        lines.append("")
        log_filename = self.log_file.name
        lines.append(f"- [è©³ç´°ãƒ­ã‚°](./{log_filename}) (è¡Œ{test_info.log_start_line}-{test_info.log_end_line})")
        
        # ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆ
        if test_info.screenshots:
            last_screenshots = test_info.screenshots[-3:]  # æœ€å¾Œã®3æš
            images_dir = f"{self.log_file.stem}_images"
            for ss in last_screenshots:
                filename = ss.get("filename", "")
                label = ss.get("label") or "Screenshot"
                lines.append(f"- [{label}](./{images_dir}/{filename})")
        
        lines.append("")
        lines.append("---")
        
        return lines


# ========================================
# ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
# ========================================

def main():
    """ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã‚¨ãƒ³ãƒˆãƒªãƒã‚¤ãƒ³ãƒˆ"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description="ãƒ†ã‚¹ãƒˆå¤±æ•—ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆãƒ„ãƒ¼ãƒ«ï¼ˆLLMåˆ†æã‚’ä½¿ç”¨ï¼‰"
    )
    parser.add_argument("log_dir", help="ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆrun_YYYYMMDD_HHMMSSï¼‰")
    parser.add_argument("--model", default="gpt-4.1-mini", help="ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«")
    
    args = parser.parse_args()
    
    try:
        generator = FailureReportGenerator(
            log_dir=Path(args.log_dir),
            model_name=args.model
        )
        
        report_path = generator.generate_report()
        print(f"ğŸ”— file://{report_path.absolute()}")
        
    except FileNotFoundError as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        return 1
    except Exception as e:
        print(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())
